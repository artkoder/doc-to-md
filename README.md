# RAG-пайплайн для корпуса по истории региона

Этот репозиторий содержит Jupyter-ноутбук, который автоматизирует цепочку:

1. Получение отсканированных книг / статей (PDF / изображения) и страниц из Telegram.
2. Растеризация и OCR (включая распознание текста с иллюстраций).
3. Семантическое разбиение текста на чанки.
4. Запись результатов в Supabase (Postgres + pgvector) для последующего RAG-поиска.
5. Генерация вспомогательных превью-файлов (`RAG_DATA_PREVIEW_*.md`) для ручной проверки.

Проект используется для построения базы знаний по истории региона (книги, статьи, каталоги и т.п.).

---

## Основные компоненты пайплайна

### 1. Телеграм-источник

- Страницы книг и статей берутся из Telegram-канала (например, канал приёмки сканов).
- Ноутбук использует **Telethon** для:
  - чтения сообщений и медиа,
  - скачивания страниц,
  - отправки результатов и превью в целевой канал.

Каждой странице соответствует `tg_msg_id` — ID сообщения в Telegram, где лежит изображение страницы.

### 2. Растеризация и OCR

В ячейках ноутбука:

- PDF / изображения разбиваются на отдельные страницы.
- Для каждой страницы:
  - определяется основной текст,
  - выделяются иллюстрации (кропы),
  - при необходимости распознаются подписи к медиа.
- Для OCR и семантических задач используется модель **Gemini** (Google Generative AI).

Результат промежуточного этапа: маркдаун-файл вида:

- `_Название_источника.md` — полный текст с разметкой;
- плюс информация о медиа (`page_media`), с привязкой к `file_id` (странице) и ссылками вида `tg://msg_id?id=...`.

### 3. Семантический чанкинг

Отдельная часть ноутбука:

- читает маркдаун,
- строит список «окон» текста (например, по 2–3 страницы с overlap),
- для каждого окна вызывает LLM для семантического разбиения на чанки.

Каждый чанк содержит:

- `chunk_content` — текст,
- `chunk_type` — контент, библиография, сноска и т.п.,
- `semantic_header` — семантический заголовок (если есть),
- `relevance_score` — оценка важности,
- `lang` — язык текста,
- `file_id`, `book_page` — привязка к исходной странице,
- `media_ids` — список Telegram `msg_id` связанных иллюстраций,
- `media_internal_ids` — внутренние ID кропов (типа `image_03_01`),
- `chunk_year_start` / `chunk_year_end` — оценка временного диапазона (если применимо).

В результате формируется объект `book_data`, включающий:

- `metadata` — метаданные книги/статьи,
- `chunks` — основные чанки,
- `bibliography` — отдельные чанки библиографии,
- `footnotes` — сноски,
- `page_media` — медиа по страницам.

---

## 4. Дедупликация (проверка дублей книг)

На раннем этапе (после распознавания первых страниц):

- собираются:
  - `isbn` (если есть),
  - метаданные (название, авторы, год),
  - несколько первых страниц (текст и/или изображения).
- выполняется трёхуровневая проверка:

1. **ISBN** — поиск точного совпадения в базе.
2. **Метаданные** — поиск кандидатов по title+year через `pg_trgm`.
3. **Сравнение по страницам (картинки)** —
   - из базы извлекаются первые страницы кандидатов (по `chunks.file_id` и `tg_msg_id`),
   - скачиваются картинки из Telegram,
   - с помощью vision-модели (например, `gemini-2.0-flash`) делается сравнение:
     «одна ли это книга или разные?».

Если дедупликация считает, что книга уже есть:

- ноутбук логирует это,
- отправляет сообщение в приёмный Telegram-канал,
- **останавливает обработку этой книги** (ничего не перезаписывает в БД).

Решение об удалении/обновлении существующей записи принимает оператор отдельным скриптом.

---

## 5. Запись в Supabase (Postgres + pgvector)

Ноутбук умеет автоматически:

1. Проверять/создавать схему БД (`ensure_db_schema()`):
   - расширения:
     - `vector`
     - `pg_trgm`
     - `pgcrypto`
   - таблицы:
     - `books`
     - `media`
     - `chunks` (с `embedding vector(768)` и индексом `hnsw` по `embedding`).
   - функцию векторного поиска:

     ```sql
     match_chunks(query_embedding vector(768), match_threshold float, match_count int)
     ```

     которая возвращает данные чанков и связанных книг.

2. Вставлять данные одной книги (`insert_book_and_related(book_data)`):
   - строку в `books` (метаданные, телеграм-канал, исходный файл, summary и т.п.),
   - строки в `media` (каждый кроп страницы с `tg_msg_id`),
   - строки в `chunks`:
     - текст чанка,
     - метаданные (страница, тип, годы, media_ids, semantic_header и т.д.),
     - вектор `embedding` по `chunk_content` (через `text-embedding-004`).

Если книга уже есть (по `isbn` или `title+year`), вставка пропускается, чтобы не трогать существующие данные.

---

## 6. Векторный поиск (match_chunks)

В базе определена универсальная SQL-функция:

```sql
match_chunks(query_embedding vector(768),
             match_threshold float,
             match_count int)
