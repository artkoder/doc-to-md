{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# –Ø—á–µ–π–∫–∞ 1: –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û–ï –û–ë–ù–û–í–õ–ï–ù–ò–ï –ë–ò–ë–õ–ò–û–¢–ï–ö\n!apt-get update -qq\n!apt-get install -y -qq djvulibre-bin tesseract-ocr tesseract-ocr-rus tesseract-ocr-deu tesseract-ocr-eng\n# –§–ª–∞–≥ -U –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ gemini-1.5-flash\n!pip install -U -q google-generativeai telethon pymupdf tqdm pillow pytesseract\n!echo \"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –û–ë–ù–û–í–õ–ï–ù–´. –¢–µ–ø–µ—Ä—å –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤–∏–¥–Ω–∞.\"","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-12-03T20:20:17.245667Z","iopub.execute_input":"2025-12-03T20:20:17.246034Z","iopub.status.idle":"2025-12-03T20:20:49.734440Z","shell.execute_reply.started":"2025-12-03T20:20:17.245998Z","shell.execute_reply":"2025-12-03T20:20:49.733415Z"},"trusted":true},"outputs":[{"name":"stdout","text":"^C\nSelecting previously unselected package djvulibre-bin.\n(Reading database ... 128639 files and directories currently installed.)\nPreparing to unpack .../0-djvulibre-bin_3.5.28-2ubuntu0.22.04.1_amd64.deb ...\nUnpacking djvulibre-bin (3.5.28-2ubuntu0.22.04.1) ...\nSelecting previously unselected package libexiv2-27:amd64.\nPreparing to unpack .../1-libexiv2-27_0.27.5-3ubuntu1_amd64.deb ...\nUnpacking libexiv2-27:amd64 (0.27.5-3ubuntu1) ...\nSelecting previously unselected package libgraphicsmagick-q16-3.\nPreparing to unpack .../2-libgraphicsmagick-q16-3_1.4+really1.3.38-1ubuntu0.1_amd64.deb ...\nUnpacking libgraphicsmagick-q16-3 (1.4+really1.3.38-1ubuntu0.1) ...\nSelecting previously unselected package libgraphicsmagick++-q16-12.\nPreparing to unpack .../3-libgraphicsmagick++-q16-12_1.4+really1.3.38-1ubuntu0.1_amd64.deb ...\nUnpacking libgraphicsmagick++-q16-12 (1.4+really1.3.38-1ubuntu0.1) ...\nSelecting previously unselected package tesseract-ocr-deu.\nPreparing to unpack .../4-tesseract-ocr-deu_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-deu (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-rus.\nPreparing to unpack .../5-tesseract-ocr-rus_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-rus (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package pdf2djvu.\nPreparing to unpack .../6-pdf2djvu_0.9.18.2-1_amd64.deb ...\nUnpacking pdf2djvu (0.9.18.2-1) ...\nSetting up libexiv2-27:amd64 (0.27.5-3ubuntu1) ...\nSetting up djvulibre-bin (3.5.28-2ubuntu0.22.04.1) ...\nSetting up libgraphicsmagick-q16-3 (1.4+really1.3.38-1ubuntu0.1) ...\nSetting up tesseract-ocr-deu (1:4.00~git30-7274cfa-1.1) ...\nSetting up libgraphicsmagick++-q16-12 (1.4+really1.3.38-1ubuntu0.1) ...\nSetting up pdf2djvu (0.9.18.2-1) ...\nSetting up tesseract-ocr-rus (1:4.00~git30-7274cfa-1.1) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m748.5/748.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pyaes (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngradio 5.38.1 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –û–ë–ù–û–í–õ–ï–ù–´. –¢–µ–ø–µ—Ä—å –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤–∏–¥–Ω–∞.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# –Ø—á–µ–π–∫–∞ –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\n\nprint(\"üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...\")\ntry:\n    user_secrets = UserSecretsClient()\n    GENAI_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n    genai.configure(api_key=GENAI_KEY)\n    \n    available_models = []\n    for m in genai.list_models():\n        if 'generateContent' in m.supported_generation_methods:\n            print(f\"   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: {m.name}\")\n            available_models.append(m.name)\n            \n    if not available_models:\n        print(\"‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API Key.\")\n    else:\n        print(f\"\\n–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(available_models)}\")\n\nexcept Exception as e:\n    print(f\"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}\")\n    print(\"üí° –°–û–í–ï–¢: –ù–∞–∂–º–∏—Ç–µ –≤ –º–µ–Ω—é 'Run' -> 'Restart Session', —á—Ç–æ–±—ã –æ–±–Ω–æ–≤–∏—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫–∏.\")","metadata":{"execution":{"iopub.status.busy":"2025-12-03T20:20:49.736249Z","iopub.execute_input":"2025-12-03T20:20:49.736927Z","iopub.status.idle":"2025-12-03T20:20:54.394835Z","shell.execute_reply.started":"2025-12-03T20:20:49.736891Z","shell.execute_reply":"2025-12-03T20:20:54.393905Z"},"trusted":true},"outputs":[{"name":"stdout","text":"üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.5-flash\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.5-pro\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.0-flash-exp\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.0-flash\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.0-flash-001\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.0-flash-exp-image-generation\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.0-flash-lite-001\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.0-flash-lite\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.0-flash-lite-preview-02-05\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.0-flash-lite-preview\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.0-pro-exp\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.0-pro-exp-02-05\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-exp-1206\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.5-flash-preview-tts\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.5-pro-preview-tts\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/learnlm-2.0-flash-experimental\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemma-3-1b-it\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemma-3-4b-it\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemma-3-12b-it\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemma-3-27b-it\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemma-3n-e4b-it\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemma-3n-e2b-it\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-flash-latest\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-flash-lite-latest\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-pro-latest\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.5-flash-lite\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.5-flash-image-preview\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.5-flash-image\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.5-flash-preview-09-2025\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.5-flash-lite-preview-09-2025\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-3-pro-preview\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-3-pro-image-preview\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/nano-banana-pro-preview\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-robotics-er-1.5-preview\n   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: models/gemini-2.5-computer-use-preview-10-2025\n\n–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: 35\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# === –Ø–ß–ï–ô–ö–ê 2: V57.1 (FULL META + JSON REPAIR) ===\nimport os\nimport time\nimport json\nimport asyncio\nimport shutil\nimport re\nimport fitz  # PyMuPDF\nimport subprocess\nimport google.generativeai as genai\nfrom PIL import Image\nimport pytesseract\nfrom datetime import datetime\nfrom telethon import TelegramClient, events, functions, types\nfrom telethon.sessions import MemorySession\nfrom kaggle_secrets import UserSecretsClient\nfrom tqdm.notebook import tqdm\nfrom google.api_core.exceptions import ResourceExhausted, ServiceUnavailable, InternalServerError\n\n# === SAFETY ===\nImage.MAX_IMAGE_PIXELS = None \n\n# === SETTINGS ===\nSOURCE_USERNAME = \"sendDoc39\"\n# –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ø—Ä–æ–±—É–µ–º 5, –ø—Ä–∏ –æ—à–∏–±–∫–µ 3, –ø—Ä–∏ —Å–æ–≤—Å–µ–º –æ—à–∏–±–∫–µ 1\nBATCH_STRATEGY = [5, 3, 1]  \n\nGLOBAL_REQUEST_DELAY = 30.0  \n\nZOOM_FACTOR = 2.0           \nMARKER_EMOJI = \"‚úÖ\"\nMAX_EMPTY_GAP = 5\nSAFETY_LIMIT = 500\n\nDJVU_TARGET_SIZE = 2560     \nCROP_PADDING = 15           \nSNAP_THRESHOLD = 30         \n\n# --- PRODUCTION SETTINGS ---\nDEBUG_PAGE_LIMIT = 11\n# ---------------------------\n\nuser_secrets = UserSecretsClient()\nTEMP_DIR = \"./temp_processing\"\nCROPS_DIR = \"./temp_crops\"\nOCR_JSON_DIR = \"./temp_ocr_json\"\n\nLOG_BUFFER = []\n\ndef log(text):\n    tqdm.write(text)\n    LOG_BUFFER.append(text)\n\nasync def send_report(client, dest_entity):\n    report_text = \"\\n\".join(LOG_BUFFER)\n    if len(report_text) > 4000: report_text = report_text[-3900:]\n    try:\n        if dest_entity: await client.send_message(dest_entity, f\"üìù **Report:**\\n\\n...{report_text}\")\n    except: pass\n\n# === JSON REPAIR KIT ===\ndef repair_json_content(text):\n    \"\"\"\n    –õ–µ—á–∏—Ç –æ—à–∏–±–∫—É 'Invalid \\escape', —ç–∫—Ä–∞–Ω–∏—Ä—É—è –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–ª–µ—à–∏,\n    –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—è –≤–∞–ª–∏–¥–Ω—ã–µ JSON-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (\\n, \\t, \\\", \\\\).\n    \"\"\"\n    # –ü–∞—Ç—Ç–µ—Ä–Ω –Ω–∞—Ö–æ–¥–∏—Ç backslash, –∑–∞ –∫–æ—Ç–æ—Ä—ã–º –ù–ï —Å–ª–µ–¥—É–µ—Ç –≤–∞–ª–∏–¥–Ω—ã–π escape-—Å–∏–º–≤–æ–ª\n    # Valid JSON escapes: \", \\, /, b, f, n, r, t, u\n    pattern = r'\\\\(?![/u\"bfnrt\\\\])'\n    return re.sub(pattern, r'\\\\\\\\', text)\n\n# === MODEL MANAGER ===\nclass ModelRotator:\n    def __init__(self, api_keys):\n        if isinstance(api_keys, str): self.api_keys = [api_keys]\n        else: self.api_keys = api_keys\n        self.current_key_idx = 0\n        self._configure_current_key()\n        self.last_request_ts = 0\n        \n        # Priority Models\n        self.models_list = [\n            \"models/gemini-2.5-flash\",      # Priority 1\n            \"models/gemini-2.0-flash-exp\",  # Priority 2\n            \"models/gemini-1.5-flash\",      # Priority 3\n        ]\n        log(f\"üß† Models initialized. Main: {self.models_list[0]}. Delay: {GLOBAL_REQUEST_DELAY}s.\")\n\n    def _configure_current_key(self):\n        genai.configure(api_key=self.api_keys[self.current_key_idx])\n\n    async def _enforce_global_pace(self):\n        now = time.time()\n        elapsed = now - self.last_request_ts\n        if elapsed < GLOBAL_REQUEST_DELAY:\n            wait_time = GLOBAL_REQUEST_DELAY - elapsed + 1\n            await asyncio.sleep(wait_time)\n        self.last_request_ts = time.time()\n\n    async def generate_content(self, content):\n        for i, model_name in enumerate(self.models_list):\n            await self._enforce_global_pace()\n            \n            # Low temp for structural adherence\n            model = genai.GenerativeModel(\n                model_name=model_name,\n                safety_settings=[{\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"}],\n                generation_config={\"response_mime_type\": \"application/json\", \"temperature\": 0.0}\n            )\n            \n            try:\n                log(f\"      üì° [{model_name}] Sending...\")\n                t_start = time.time()\n                response = await asyncio.to_thread(model.generate_content, content)\n                dur = time.time() - t_start\n                \n                usage = {\"in\": 0, \"out\": 0}\n                if response.usage_metadata:\n                    usage[\"in\"] = response.usage_metadata.prompt_token_count\n                    usage[\"out\"] = response.usage_metadata.candidates_token_count\n                \n                return response.text, usage, model_name, dur\n            \n            except (ResourceExhausted, ServiceUnavailable, InternalServerError) as e:\n                log(f\"      ‚ö†Ô∏è Limit/Error on {model_name}.\")\n                if i == len(self.models_list) - 1:\n                    log(\"      ‚ùå ALL MODELS EXHAUSTED. Aborting execution.\")\n                    raise e \n                log(\"      ‚û°Ô∏è Switching to backup model...\")\n                continue\n                \n            except Exception as e:\n                if \"429\" in str(e):\n                    log(f\"      ‚ö†Ô∏è 429 Too Many Requests on {model_name}.\")\n                    if i == len(self.models_list) - 1:\n                        log(\"      ‚ùå ALL MODELS EXHAUSTED (429). Aborting.\")\n                        raise e\n                    continue\n                log(f\"      ‚ùå Unexpected Error ({model_name}): {e}\")\n                raise e\n\n        raise Exception(\"Unexpected logic flow in ModelRotator\")\n\n# === INIT ===\nmodel_rotator = None\ntry:\n    GENAI_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n    model_rotator = ModelRotator(GENAI_KEY)\nexcept Exception as e: log(f\"‚ùå Init Error: {e}\")\n\n# === AI LOGIC (UPDATED PROMPT V57.1) ===\ndef get_gemini_prompt(is_first_batches, previous_context=None):\n    base_prompt = \"\"\"\n    TASK: OCR & Layout Analysis (Spread vs Journal Page) with Rich Formatting.\n    \"\"\"\n    \n    if previous_context:\n        base_prompt += f\"\"\"\n    CONTEXT FROM PREV BATCH: \"...{previous_context}...\"\n    \"\"\"\n\n    base_prompt += \"\"\"\n    OUTPUT FORMAT (JSON):\n    {\n      \"pages\": [\n        {\n           \"book_page_num\": \"string\",         // PRINTED page number.\n           \"classification_code\": \"string\",   // UDC/BBC codes (e.g. \"–£–î–ö...\").\n           \"main_text\": \"string\",             // Markdown text with headers/bold/italic.\n           \"footnotes\": \"string\",             // Isolated footnotes.\n           \"marginalia\": \"string\",            // Headers/Running titles.\n           \"media_objects\": [\n              { \"id\": \"string\", \"coordinates\": [y,x,y,x], \"type\": \"string\", \"caption_ru\": \"string\" }\n           ]\n        }\n      ],\n    \"\"\"\n    \n    if is_first_batches:\n        base_prompt += \"\"\"\n      \"bibliographic_data\": {\n          \"title\": \"string\",\n          \"authors\": [\"string\"],\n          \"co_authors\": [\"string\"],\n          \"publisher\": \"string\",\n          \"year\": \"string\",\n          \"isbn\": \"string\",\n          \"document_type\": \"string (book, article, official_document, archive_record, other)\",\n          \"historical_period_description\": \"string (–Ω–∞ —Ä—É—Å—Å–∫–æ–º)\",\n          \"historical_year_start\": \"int or null\",\n          \"historical_year_end\": \"int or null\",\n          \"summary\": \"string (–Ω–∞ —Ä—É—Å—Å–∫–æ–º)\"\n      }\n    }\n    \"\"\"\n    else:\n        base_prompt += '  \"batch_metadata\": { \"status\": \"cont\" }\\n}'\n\n    # --- RULES V57.1 ---\n    base_prompt += \"\"\"\n    CRITICAL RULES:\n    \n    1. **LAYOUT & ZONES**:\n       - Check for **Single Page** vs **Spread** (2 pages).\n       - If **Single Page** (Journal Style): Detect horizontal zones (Top Article end / Bottom Article start). Use `***` separator.\n       - Do not merge columns across different articles.\n\n    2. **RICH MARKDOWN FORMATTING (HIERARCHY)**:\n       - **Headers**: Detect font size/boldness. Use `##` for Main Titles, `###` for Subtitles.\n       - **Emphasis**: Use `**bold**` for bold text. Use `*italic*` for italics.\n       - **Structure**: Preserve paragraph breaks.\n\n    3. **METADATA**:\n       - Extract full bibliographic data if available (first batch).\n       - Use **Russian language** for descriptions and summaries.\n       - If a value is unknown, use `null` or empty string.\n\n    4. **FOOTNOTES**:\n       - CUT footnote text from main body and PASTE into `footnotes`.\n       \n    5. **JSON SAFETY**:\n       - Do not use raw backslashes inside strings unless escaping. Avoid LaTeX macros if possible, use Unicode.\n    \"\"\"\n    return base_prompt\n\nasync def process_single_batch(image_paths, page_nums, prev_ctx_text):\n    if not model_rotator: raise Exception(\"No models\")\n    \n    is_first_batch = (page_nums[0] == 1)\n    \n    prompt = get_gemini_prompt(is_first_batch, prev_ctx_text)\n    content = [prompt]\n    loaded_imgs = []\n    \n    for p in image_paths:\n        img = Image.open(p)\n        w, h = img.size\n        if w > 3072 or h > 3072:\n            img.thumbnail((3072, 3072), Image.Resampling.LANCZOS)\n        loaded_imgs.append(img)\n        content.append(img)\n        \n    text_resp, usage, model_name, duration = await model_rotator.generate_content(content)\n    \n    clean_text = text_resp.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n    if clean_text.startswith(\"json\"): clean_text = clean_text[4:].strip()\n    match = re.search(r'\\{.*\\}', clean_text, re.DOTALL)\n    if match: clean_text = match.group(0)\n\n    try:\n        # 1. –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ \"–∫–∞–∫ –µ—Å—Ç—å\"\n        data = json.loads(clean_text) \n    except Exception as e1:\n        # 2. –ü–æ–ø—ã—Ç–∫–∞ –ª–µ—á–µ–Ω–∏—è JSON (backslashes)\n        log(f\"      ‚ö†Ô∏è JSON Error 1: {e1}. Repairing...\")\n        repaired_text = repair_json_content(clean_text)\n        try:\n            data = json.loads(repaired_text)\n            log(\"      üõ†Ô∏è Repair Successful!\")\n        except Exception as e2:\n            log(f\"      ‚ùå Repair Failed: {e2}. Raw text len: {len(clean_text)}\")\n            raise Exception(\"JSON Parsing Failed completely\") \n    \n    batch_meta = data.get(\"bibliographic_data\", data.get(\"batch_metadata\", None))\n    return data.get(\"pages\", []), loaded_imgs, usage, model_name, duration, batch_meta\n\ndef crop_images(pil_imgs, pages_data):\n    crops = []\n    if not os.path.exists(CROPS_DIR): os.makedirs(CROPS_DIR, exist_ok=True)\n    \n    for i, page_data in enumerate(pages_data):\n        if i >= len(pil_imgs): src = pil_imgs[-1]\n        else: src = pil_imgs[i]\n        \n        media_list = page_data.get(\"media_objects\", [])\n        w, h = src.size\n        \n        # Cover Guard\n        is_cover_page = False\n        cover_object = None\n        for m in media_list:\n            if m.get(\"type\") == \"cover\" or m.get(\"id\") == \"book_cover\":\n                is_cover_page = True\n                cover_object = m\n                break\n        \n        if is_cover_page and cover_object:\n            cover_object[\"coordinates\"] = [0, 0, 1000, 1000]\n            cover_object[\"id\"] = \"book_cover\"\n            media_list = [cover_object]\n        \n        for meta in media_list:\n            try:\n                ymin, xmin, ymax, xmax = meta[\"coordinates\"]\n                left = (xmin/1000)*w - CROP_PADDING\n                top = (ymin/1000)*h - CROP_PADDING\n                right = (xmax/1000)*w + CROP_PADDING\n                bottom = (ymax/1000)*h + CROP_PADDING\n                \n                left = max(0, left); top = max(0, top)\n                right = min(w, right); bottom = min(h, bottom)\n                \n                if (right-left < 30) or (bottom-top < 30): continue\n                \n                crop = src.crop((left, top, right, bottom))\n                \n                raw_id = meta.get(\"id\", f\"media_{int(time.time())}_{i}\")\n                if meta.get(\"type\") == \"cover\": media_id = \"book_cover\"\n                else: media_id = re.sub(r'[^a-zA-Z0-9_]', '', raw_id)\n                \n                c_name = f\"{media_id}.png\"\n                c_path = os.path.join(CROPS_DIR, c_name)\n                crop.save(c_path)\n                \n                caption = f\"{meta.get('type', 'img').upper()}: {meta.get('caption_ru', '')}\"\n                crops.append((c_path, caption, media_id))\n            except: pass\n    return crops\n\n# === TELEGRAM & UTILS ===\nasync def get_last_id(client, entity):\n    try:\n        full = await client(functions.channels.GetFullChannelRequest(entity))\n        match = re.search(r\"\\[LastID:\\s*(\\d+)\\]\", full.full_chat.about or \"\")\n        if match: return int(match.group(1))\n    except: pass\n    return None\n\nasync def update_last_id(client, entity, new_id):\n    try:\n        full = await client(functions.channels.GetFullChannelRequest(entity))\n        about = full.full_chat.about or \"\"\n        tag = f\"[LastID: {new_id}]\"\n        if \"[LastID:\" in about: new_about = re.sub(r\"\\[LastID:\\s*\\d+\\]\", tag, about)\n        else: new_about = f\"{about}\\n\\n{tag}\".strip()\n        if new_about != about: await client(functions.messages.EditChatAboutRequest(peer=entity, about=new_about))\n    except: pass\n\nasync def is_already_processed(message):\n    try:\n        if not message.reactions: return False\n        for r in message.reactions.results:\n            emoji = r.reaction.emoticon if hasattr(r.reaction, 'emoticon') else str(r.reaction)\n            if emoji == MARKER_EMOJI and r.chosen: return True\n    except: pass\n    return False\n\nasync def mark_processed(client, msg):\n    try:\n        await client(functions.messages.SendReactionRequest(peer=msg.peer_id, msg_id=msg.id, reaction=[types.ReactionEmoji(emoticon=MARKER_EMOJI)]))\n    except: pass\n\ndef render_pdf(path):\n    doc = fitz.open(path)\n    mat = fitz.Matrix(ZOOM_FACTOR, ZOOM_FACTOR)\n    limit = len(doc)\n    if DEBUG_PAGE_LIMIT:\n        limit = min(len(doc), DEBUG_PAGE_LIMIT)\n    for i in range(limit):\n        page = doc.load_page(i)\n        pix = page.get_pixmap(matrix=mat, alpha=False)\n        out = os.path.join(TEMP_DIR, f\"page_{i+1:04d}.png\")\n        pix.save(out)\n        yield out, i+1, len(doc)\n    doc.close()\n\ndef render_djvu(path):\n    safe = os.path.join(TEMP_DIR, \"temp.djvu\")\n    shutil.move(path, safe)\n    try:\n        subprocess.run([\n            'ddjvu', '-format=tiff', f'-size={DJVU_TARGET_SIZE}x{DJVU_TARGET_SIZE}', \n            '-eachpage', safe, os.path.join(TEMP_DIR, \"p_%04d.tif\")\n        ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n    except Exception as e:\n        log(f\"‚ùå DjVu Render Error: {e}\")\n        return\n\n    files = sorted([f for f in os.listdir(TEMP_DIR) if f.endswith(\".tif\")])\n    if DEBUG_PAGE_LIMIT: files = files[:DEBUG_PAGE_LIMIT]\n    for i, f in enumerate(tqdm(files, desc=\"Conv\", unit=\"pg\", leave=False)):\n        full = os.path.join(TEMP_DIR, f)\n        try:\n            with Image.open(full) as img:\n                png = full.replace(\".tif\", \".png\")\n                img.save(png, \"PNG\")\n                yield png, i+1, len(files)\n            os.remove(full)\n        except: pass\n    if os.path.exists(safe): os.remove(safe)\n\n\n# === OCR (TESSERACT) ===\nasync def run_ocr_for_pages(client, dest_entity, book_title, pages, global_resources=None):\n    \"\"\"\n    –í—ã–ø–æ–ª–Ω—è–µ—Ç OCR –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–º–æ—â—å—é Tesseract –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç\n    JSON —Å –º–∞—Å—Å–∏–≤–æ–º —Å–ª–æ–≤ (text, bbox, conf) –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ.\n    –ó–∞—Ç–µ–º –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç JSON-—Ñ–∞–π–ª—ã –≤ Telegram –±–∞—Ç—á–∞–º–∏ –ø–æ 5 —à—Ç—É–∫\n    –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å—Å—ã–ª–∫–∏ –≤ global_resources.\n    \"\"\"\n    if not pages or client is None or dest_entity is None:\n        return\n\n    base = os.path.splitext(book_title)[0]\n    safe_base = re.sub(r\"[^0-9A-Za-z–ê-–Ø–∞-—è—ë–Å_-]+\", \"_\", base)[:80]\n\n    from pytesseract import Output\n\n    json_items = []\n    total_pages = len(pages)\n    log(f\"   üîç OCR (Tesseract rus+deu+eng) for {total_pages} pages...\")\n\n    for local_idx, (img_path, page_num, total_in_doc) in enumerate(pages, start=1):\n        try:\n            log(f\"      üìù OCR page {page_num}/{total_in_doc} ({local_idx}/{total_pages})...\")\n            img = Image.open(img_path)\n            data = pytesseract.image_to_data(\n                img,\n                lang=\"rus+deu+eng\",\n                output_type=Output.DICT\n            )\n\n            words = []\n            n = len(data.get(\"text\", []))\n            for i in range(n):\n                text = (data[\"text\"][i] or \"\").strip()\n                if not text:\n                    continue\n\n                conf_raw = data.get(\"conf\", [\"0\"])[i]\n                try:\n                    conf = float(conf_raw)\n                except Exception:\n                    conf = -1.0\n                if conf < 0:\n                    continue\n\n                left = int(data.get(\"left\", [0])[i])\n                top = int(data.get(\"top\", [0])[i])\n                width = int(data.get(\"width\", [0])[i])\n                height = int(data.get(\"height\", [0])[i])\n\n                words.append({\n                    \"text\": text,\n                    \"bbox\": {\"x\": left, \"y\": top, \"w\": width, \"h\": height},\n                    \"conf\": conf\n                })\n\n            payload = {\n                \"file_name\": book_title,\n                \"safe_name\": safe_base,\n                \"page_index\": page_num,\n                \"page_in_doc\": page_num,\n                \"total_pages\": total_in_doc,\n                \"words\": words\n            }\n\n            json_name = f\"{safe_base}_p{page_num:04d}_words.json\"\n            json_path = os.path.join(OCR_JSON_DIR, json_name)\n            with open(json_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(payload, f, ensure_ascii=False)\n\n            json_items.append({\n                \"page_num\": page_num,\n                \"total_pages\": total_in_doc,\n                \"json_path\": json_path,\n                \"json_name\": json_name,\n                \"tg_msg_id\": None\n            })\n\n        except Exception as e:\n            log(f\"      ‚ö†Ô∏è OCR error on page {page_num}: {str(e)[:100]}\")\n            continue\n\n    if not json_items:\n        log(\"   ‚ö†Ô∏è OCR produced no JSON files.\")\n        return\n\n    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –±–∞—Ç—á–∞–º–∏ –ø–æ 5 —Ñ–∞–π–ª–æ–≤\n    BATCH_SIZE = 5\n    for i in range(0, len(json_items), BATCH_SIZE):\n        batch = json_items[i:i + BATCH_SIZE]\n        try:\n            batch_idx = i // BATCH_SIZE + 1\n            log(f\"   üì§ Sending OCR JSON batch {batch_idx} ({len(batch)} files) to Telegram...\")\n            paths = [b[\"json_path\"] for b in batch]\n            caption = f\"üìÑ OCR words [{safe_base}] batch {batch_idx}\"\n            result = await client.send_file(dest_entity, paths, caption=caption, force_document=True)\n\n            # Telethon –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –æ–¥–∏–Ω–æ—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–ª–∏ —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π\n            if isinstance(result, (list, tuple)):\n                msgs = list(result)\n            else:\n                msgs = [result]\n\n            for meta, msg in zip(batch, msgs):\n                meta[\"tg_msg_id\"] = getattr(msg, \"id\", None)\n\n        except Exception as e:\n            log(f\"   ‚ö†Ô∏è Failed to send OCR JSON batch: {str(e)[:120]}\")\n            continue\n\n    # –î–æ–±–∞–≤–ª—è–µ–º OCR-—Ä–µ—Å—É—Ä—Å—ã –≤ global_resources, —á—Ç–æ–±—ã –æ–Ω–∏ –ø–æ–ø–∞–ª–∏ –≤ MD\n    if global_resources is not None:\n        for it in json_items:\n            msg_id = it.get(\"tg_msg_id\")\n            if not msg_id:\n                continue\n            page_num = it[\"page_num\"]\n            res_id = f\"ocr_{page_num:04d}_words\"\n            caption = f\"OCR words page {page_num}\"\n            link = f\"tg://msg_id?id={msg_id}\"\n            global_resources.append({\"id\": res_id, \"caption\": caption, \"link\": link})\n\n    # –ß–∏—Å—Ç–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ JSON-—Ñ–∞–π–ª—ã\n    for it in json_items:\n        try:\n            os.remove(it[\"json_path\"])\n        except Exception:\n            pass\n\n# === MAIN ===\n# === MAIN ===\nasync def main_logic():\n    log(f\"üöÄ **Launch V57.1 (Full Meta + JSON Repair):** {datetime.now().strftime('%H:%M:%S')}\")\n    \n    try:\n        API_ID = int(user_secrets.get_secret(\"TELEGRAM_API_ID\"))\n        API_HASH = user_secrets.get_secret(\"TELEGRAM_API_HASH\")\n        BOT_TOKEN = user_secrets.get_secret(\"TELEGRAM_BOT_TOKEN\")\n        DEST_ID = int(user_secrets.get_secret(\"DEST_CHANNEL_ID\"))\n    except: return log(\"‚ùå Secrets missing.\")\n\n    for d in [TEMP_DIR, CROPS_DIR, OCR_JSON_DIR]:\n        if not os.path.exists(d): os.makedirs(d, exist_ok=True)\n\n    client = TelegramClient(MemorySession(), API_ID, API_HASH)\n    await client.start(bot_token=BOT_TOKEN)\n    log(\"‚úÖ Telegram OK.\")\n    if not model_rotator: return await client.disconnect()\n\n    try:\n        src_ent = await client.get_entity(SOURCE_USERNAME)\n        dst_ent = await client.get_entity(DEST_ID)\n    except: return await client.disconnect()\n\n    start_id = await get_last_id(client, src_ent) or 1\n    curr_id = start_id + 1\n    processed = 0\n    empty_streak = 0\n    t_in, t_out = 0, 0\n    pbar = tqdm(total=SAFETY_LIMIT, desc=\"Scan\", unit=\"msg\")\n\n    while processed < SAFETY_LIMIT:\n        if empty_streak >= MAX_EMPTY_GAP: log(\"üèÅ End.\"); break\n        try:\n            msgs = await client.get_messages(src_ent, ids=[curr_id])\n            msg = msgs[0] if msgs else None\n            \n            if not msg or isinstance(msg, types.MessageEmpty):\n                empty_streak += 1; curr_id += 1; pbar.update(1); continue\n            \n            if await is_already_processed(msg):\n                await update_last_id(client, src_ent, msg.id)\n                curr_id += 1; pbar.update(1); continue\n\n            fname = getattr(msg.file, 'name', None)\n            if not fname: curr_id += 1; pbar.update(1); continue\n            ext = fname.split('.')[-1].lower()\n            if ext not in ['pdf', 'djvu']: curr_id += 1; pbar.update(1); continue\n\n            empty_streak = 0\n            log(f\"\\nüìò **{fname}** (ID: {msg.id})\")\n            fpath = os.path.join(TEMP_DIR, fname)\n            await msg.download_media(file=fpath)\n            \n            iter_pages = render_pdf(fpath) if ext == 'pdf' else render_djvu(fpath)\n            \n            # === FULL METADATA STRUCT V57 ===\n            full_bibliographic_data = {\n                \"isbn\": \"\",\n                \"title\": \"\",\n                \"authors\": [],\n                \"co_authors\": [],\n                \"publisher\": \"\",\n                \"year\": \"\",\n                \"document_type\": \"\",\n                \"historical_period_description\": \"\",\n                \"historical_year_start\": \"\",\n                \"historical_year_end\": \"\",\n                \"summary\": \"\",\n                \"source_telegram_channel_id\": str(dst_ent.id),\n                \"source_origin_msg_id\": str(msg.id)\n            }\n            \n            global_text_blocks = []      \n            global_resources = []        \n            global_footnotes = []\n            global_marginalia = set()\n            \n            last_batch_tail_text = None \n            current_batch_size = BATCH_STRATEGY[0]\n            \n            generated_pages_buffer = []\n            for p in iter_pages: generated_pages_buffer.append(p)\n\n            # === OCR STEP: word-level boxes to JSON ===\n            try:\n                await run_ocr_for_pages(client, dst_ent, fname, generated_pages_buffer, global_resources)\n            except Exception as e:\n                log(f\"   ‚ö†Ô∏è OCR step failed: {str(e)[:120]}\")\n\n            \n            cursor = 0\n            total_pages = len(generated_pages_buffer)\n            book_failed = False\n\n            while cursor < total_pages:\n                end = min(cursor + current_batch_size, total_pages)\n                current_slice = generated_pages_buffer[cursor : end]\n                slice_paths = [x[0] for x in current_slice]\n                slice_idxs = [x[1] for x in current_slice]\n                \n                log(f\"   üîÑ Batch {slice_idxs[0]}-{slice_idxs[-1]} (Size: {len(slice_paths)})...\")\n                \n                try:\n                    pages_list, imgs, usg, m_name, duration, meta = await process_single_batch(\n                        slice_paths, slice_idxs, last_batch_tail_text\n                    )\n                    \n                    log(f\"      ‚úÖ [{m_name}] OK in {duration:.1f}s\")\n                    t_in += usg.get(\"in\",0); t_out += usg.get(\"out\",0)\n                    \n                    # === BATCH UPLOAD ===\n                    page_msg_ids = {} \n                    ALBUM_CHUNK_SIZE = 10\n                    for i in range(0, len(slice_paths), ALBUM_CHUNK_SIZE):\n                        sub_paths = slice_paths[i : i + ALBUM_CHUNK_SIZE]\n                        sub_idxs = slice_idxs[i : i + ALBUM_CHUNK_SIZE]\n                        try:\n                            sent_msgs = await client.send_file(dst_ent, file=sub_paths, force_document=True)\n                            if not isinstance(sent_msgs, list): sent_msgs = [sent_msgs]\n                            for sent_m, p_idx in zip(sent_msgs, sub_idxs):\n                                page_msg_ids[p_idx] = sent_m.id\n                            await asyncio.sleep(2) \n                        except: pass\n                    # ====================\n\n                    if meta and isinstance(meta, dict):\n                        for k, v in meta.items():\n                            if k in full_bibliographic_data and v:\n                                # Overwrite empty fields, don't overwrite existing valid data if new one is partial\n                                curr = full_bibliographic_data[k]\n                                if not curr: full_bibliographic_data[k] = v\n\n                    crops = crop_images(imgs, pages_list)\n                    for cp_path, cp_cap, cp_id in crops:\n                        msg_link = \"local_error\"\n                        if os.path.exists(cp_path):\n                            try:\n                                c_msg = await client.send_file(dst_ent, cp_path, caption=f\"üñº {cp_cap}\\nüÜî {cp_id}\", force_document=False)\n                                msg_link = f\"tg://msg_id?id={c_msg.id}\"\n                            except: pass\n                            finally:\n                                if os.path.exists(cp_path): os.remove(cp_path)\n                                await asyncio.sleep(2)\n                        global_resources.append({\"id\": cp_id, \"caption\": cp_cap, \"link\": msg_link})\n\n                    current_img_idx = 0\n                    for i, page_data in enumerate(pages_list):\n                        if i < len(slice_idxs): container_p = slice_idxs[i]\n                        else: container_p = slice_idxs[-1]\n                        \n                        raw_text = page_data.get(\"main_text\", \"\")\n                        book_p = page_data.get(\"book_page_num\", None)\n                        class_code = page_data.get(\"classification_code\", None)\n                        \n                        tg_ref_id = page_msg_ids.get(container_p, \"N/A\")\n                        \n                        if page_data.get(\"footnotes\"): \n                            global_footnotes.append(f\"[{container_p}] {page_data['footnotes']}\")\n                        \n                        if page_data.get(\"marginalia\"): \n                            cleaned_marg = page_data[\"marginalia\"].strip()\n                            if len(cleaned_marg) > 3: global_marginalia.add(cleaned_marg)\n                        \n                        final_page_text = raw_text.strip()\n                        if class_code:\n                            final_page_text = f\"**{class_code}**\\n\\n\" + final_page_text\n\n                        global_text_blocks.append({\n                            \"container_idx\": container_p,\n                            \"book_num\": book_p,\n                            \"tg_ref\": tg_ref_id, \n                            \"text\": final_page_text\n                        })\n                    \n                    if global_text_blocks:\n                        last_text_content = global_text_blocks[-1][\"text\"]\n                        last_batch_tail_text = last_text_content[-1000:]\n                    \n                    for p in slice_paths:\n                        if os.path.exists(p): os.remove(p)\n                        \n                    cursor += len(slice_paths)\n                    \n                except Exception as e:\n                    log(f\"      ‚ö†Ô∏è Failed: {str(e)[:100]}\")\n                    if \"ResourceExhausted\" in str(e) or \"429\" in str(e):\n                        log(\"      ‚ùå CRITICAL API LIMIT. Stopping book.\")\n                        book_failed = True\n                        break\n                    \n                    new_size = current_batch_size\n                    for s in BATCH_STRATEGY:\n                        if s < current_batch_size:\n                            new_size = s\n                            break\n                    if new_size == current_batch_size:\n                        cursor += 1 \n                        current_batch_size = 1\n                    else:\n                        log(f\"      ‚¨áÔ∏è Reducing batch size: {current_batch_size} -> {new_size}\")\n                        current_batch_size = new_size\n\n            if book_failed:\n                await client.send_message(dst_ent, f\"‚ùå **Aborted:** {fname} (API Limits)\")\n                await update_last_id(client, src_ent, msg.id)\n                continue\n\n            # === GLOBAL ASSEMBLY ===\n            final_md = f\"# {fname}\\n\\n\"\n            \n            final_md += \"## Metadata\\n\"\n            # Explicit order of ALL keys\n            meta_order = [\n                \"isbn\", \"title\", \"authors\", \"co_authors\", \"publisher\", \"year\", \n                \"document_type\", \"historical_period_description\", \n                \"historical_year_start\", \"historical_year_end\", \"summary\",\n                \"source_telegram_channel_id\", \"source_origin_msg_id\"\n            ]\n            \n            for k in meta_order:\n                val = full_bibliographic_data.get(k, \"\")\n                if isinstance(val, list):\n                    val_str = \", \".join([str(x) for x in val if x]) # Filter empty strings\n                else:\n                    val_str = str(val) if val is not None else \"\"\n                \n                # Print key even if empty, but NO \"N/A\"\n                final_md += f\"- **{k}**: {val_str}\\n\"\n            \n            final_md += \"\\n## Content\\n\\n\"\n\n\n            # Build mapping: scan page index -> OCR JSON Telegram msg IDs (1..N per page)\n            ocr_page_map = {}\n            if global_resources:\n                for res in global_resources:\n                    rid = str(res.get(\"id\") or \"\")\n                    # We only care about OCR JSON entries like 'ocr_0001_words'\n                    if rid.lower().startswith(\"ocr_\"):\n                        m_page = re.search(r\"_(\\d+)(?:_|$)\", rid)\n                        if not m_page:\n                            continue\n                        try:\n                            page_idx = int(m_page.group(1))\n                        except ValueError:\n                            continue\n                        link = str(res.get(\"link\") or \"\")\n                        m_id = re.search(r\"id=(\\d+)\", link)\n                        if not m_id:\n                            continue\n                        ocr_msg_id = m_id.group(1)\n                        ocr_page_map.setdefault(page_idx, []).append(ocr_msg_id)\n\n            joined_content = \"\"\n            for i, block_data in enumerate(global_text_blocks):\n                c_idx = block_data[\"container_idx\"]\n                b_num = block_data[\"book_num\"]\n                tg_id = block_data.get(\"tg_ref\", \"N/A\")\n                text = block_data[\"text\"]\n\n                # Optional OCR JSON ids for this scan page (take up to 2 just in case)\n                ocr_list = ocr_page_map.get(int(c_idx)) if c_idx is not None else None\n                ocr_suffix = \"\"\n                if ocr_list:\n                    ocr_suffix = \" | OCRjson: \" + \",\".join(str(x) for x in ocr_list[:2])\n\n                label = f\"> `[File: {c_idx} | Ref: {tg_id}{ocr_suffix}]`\"\n                if b_num:\n                    label = f\"> `[File: {c_idx} | Book: {b_num} | Ref: {tg_id}{ocr_suffix}]`\"\n\n                page_marker = f\"\\n\\n{label}\\n\"\n                if i > 0: \n                    joined_content += page_marker + text\n                else: \n                    joined_content += f\"{label}\\n\" + text\n\n            final_md += joined_content.strip()\n\n            if global_resources:\n                final_md += \"\\n\\n## APPX: Resources\\n\"\n\n                for res in global_resources:\n                    final_md += f\"- **ID**: `{res['id']}`\\n\"\n                    final_md += f\"  - **Caption**: {res['caption']}\\n\"\n                    final_md += f\"  - **Link**: {res['link']}\\n\"\n            \n            if global_footnotes:\n                final_md += \"\\n\\n## APPX: Footnotes\\n\" + \"\\n\\n\".join(global_footnotes)\n            \n            if global_marginalia:\n                final_md += \"\\n\\n## APPX: Marginalia\\n\" + \"\\n\".join(sorted(list(global_marginalia)))\n\n            md_out = os.path.join(TEMP_DIR, f\"{os.path.splitext(fname)[0]}.md\")\n            with open(md_out, \"w\", encoding=\"utf-8\") as f: f.write(final_md)\n            await client.send_file(dst_ent, md_out, caption=f\"üìö **MD FULL** (Tok: {t_in+t_out})\", force_document=True)\n            \n            await mark_processed(client, msg)\n            await update_last_id(client, src_ent, msg.id)\n            log(f\"   üéâ Done.\")\n\n        except Exception as e:\n            log(f\"‚ùå Error ID {curr_id}: {e}\")\n            import traceback; traceback.print_exc()\n        \n        finally:\n            if os.path.exists(CROPS_DIR):\n                shutil.rmtree(CROPS_DIR)\n                os.makedirs(CROPS_DIR, exist_ok=True)\n            if os.path.exists(TEMP_DIR):\n                for f in os.listdir(TEMP_DIR):\n                    if not f.endswith(\".md\"):\n                        try: os.remove(os.path.join(TEMP_DIR, f))\n                        except: pass\n                        \n        curr_id += 1; processed += 1; pbar.update(1)\n\n    log(f\"\\nüèÅ End. Total Tokens: {t_in}/{t_out}\")\n    await send_report(client, dst_ent)\n    await client.disconnect()\n\n# === RUN ===\nprint(\"üü° Pre-flight check...\")\ntry:\n    _ = UserSecretsClient().get_secret(\"TELEGRAM_API_ID\")\n    print(\"üü¢ Starting main_logic...\")\n    await main_logic()\nexcept Exception as e: print(f\"üî¥ FAIL: {e}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-03T20:20:54.395883Z","iopub.execute_input":"2025-12-03T20:20:54.396299Z","iopub.status.idle":"2025-12-03T20:25:32.385504Z","shell.execute_reply.started":"2025-12-03T20:20:54.396276Z","shell.execute_reply":"2025-12-03T20:25:32.384250Z"},"trusted":true},"outputs":[{"name":"stdout","text":"üß† Models initialized. Main: models/gemini-2.5-flash. Delay: 30.0s.\nüü° Pre-flight check...\nüü¢ Starting main_logic...\nüöÄ **Launch V57.1 (Full Meta + JSON Repair):** 20:20:55\n‚úÖ Telegram OK.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Scan:   0%|          | 0/500 [00:00<?, ?msg/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2be6971586654ed8be2db1be79a05bc1"}},"metadata":{}},{"name":"stdout","text":"\nüìò **–ü–æ–ø–æ–≤_–ú_,_–ü–æ–ø–∞–¥–∏–Ω_–ê_–û—Ç_–ö–∞–Ω—Ç–∞_–¥–æ_–¢–≤–µ—Ä–¥–æ–≥–æ_—Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è_–¥–∞–ª–µ–π,_–∫_–ª–∞–Ω–¥—à–∞—Ñ—Ç—É.pdf** (ID: 114)\n   üîç OCR (Tesseract rus+deu+eng) for 11 pages...\n      üìù OCR page 1/18 (1/11)...\n      üìù OCR page 2/18 (2/11)...\n      üìù OCR page 3/18 (3/11)...\n      üìù OCR page 4/18 (4/11)...\n      üìù OCR page 5/18 (5/11)...\n      üìù OCR page 6/18 (6/11)...\n      üìù OCR page 7/18 (7/11)...\n      üìù OCR page 8/18 (8/11)...\n      üìù OCR page 9/18 (9/11)...\n      üìù OCR page 10/18 (10/11)...\n      üìù OCR page 11/18 (11/11)...\n   üì§ Sending OCR JSON batch 1 (5 files) to Telegram...\n   üì§ Sending OCR JSON batch 2 (5 files) to Telegram...\n   üì§ Sending OCR JSON batch 3 (1 files) to Telegram...\n   üîÑ Batch 1-5 (Size: 5)...\n      üì° [models/gemini-2.5-flash] Sending...\n      ‚úÖ [models/gemini-2.5-flash] OK in 40.6s\n   üîÑ Batch 6-10 (Size: 5)...\n      üì° [models/gemini-2.5-flash] Sending...\n      ‚úÖ [models/gemini-2.5-flash] OK in 17.3s\n      ‚ö†Ô∏è Failed: 'NoneType' object has no attribute 'strip'\n      ‚¨áÔ∏è Reducing batch size: 5 -> 3\n   üîÑ Batch 6-8 (Size: 3)...\n      üì° [models/gemini-2.5-flash] Sending...\n      ‚úÖ [models/gemini-2.5-flash] OK in 24.8s\n   üîÑ Batch 9-11 (Size: 3)...\n      üì° [models/gemini-2.5-flash] Sending...\n      ‚úÖ [models/gemini-2.5-flash] OK in 22.0s\n   üéâ Done.\nüèÅ End.\n\nüèÅ End. Total Tokens: 7083/9688\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# === –Ø–ß–ï–ô–ö–ê 3: V60.1 (FIXED CPU LOOP IN OVERLAP LOGIC) ===\n\nimport os\nimport time\nimport json\nimport asyncio\nimport glob\nimport re\nimport google.generativeai as genai\nfrom datetime import datetime\nfrom kaggle_secrets import UserSecretsClient\nfrom telethon import TelegramClient\nfrom telethon.sessions import MemorySession\nimport psycopg2\nfrom psycopg2.extras import execute_values\n\n# === SETTINGS ===\nINPUT_DIR = \"./temp_processing\"\nOUTPUT_PREVIEW_FILE = \"RAG_DATA_PREVIEW.md\"\n\n# API Safety\nGLOBAL_REQUEST_DELAY = 30.0  \nTEXT_WINDOW_SIZE = 8000\nWINDOW_OVERLAP = 1000  \n\n# Chunk Constraints\nMAX_CHARS_PER_CHUNK = 1400 \n\n# Logic\nRELEVANCE_THRESHOLD = 0.3 \n\n# Debug Limit\nBOOKS_LIMIT = 1 \n\n# === AUTH SETUP ===\nuser_secrets = UserSecretsClient()\ntry:\n    GENAI_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n    genai.configure(api_key=GENAI_KEY)\n    \n    API_ID = int(user_secrets.get_secret(\"TELEGRAM_API_ID\"))\n    API_HASH = user_secrets.get_secret(\"TELEGRAM_API_HASH\")\n    BOT_TOKEN = user_secrets.get_secret(\"TELEGRAM_BOT_TOKEN\")\n    DEST_ID = int(user_secrets.get_secret(\"DEST_CHANNEL_ID\"))\nexcept:\n    print(\"‚ùå Secrets missing\")\n\nMODEL_NAME = \"models/gemini-2.5-flash\"\n\n# === UTILS ===\nclass RateLimiter:\n    def __init__(self, delay):\n        self.delay = delay\n        self.last_call = 0\n    \n    async def wait(self):\n        now = time.time()\n        elapsed = now - self.last_call\n        if elapsed < self.delay:\n            wait_time = self.delay - elapsed + 1\n            print(f\"      ‚è≥ Pace: waiting {wait_time:.1f}s...\")\n            await asyncio.sleep(wait_time)\n        self.last_call = time.time()\n\nlimiter = RateLimiter(GLOBAL_REQUEST_DELAY)\n\ndef get_flag_emoji(lang_code):\n    flags = {\n        \"ru\": \"üá∑üá∫\", \"de\": \"üá©üá™\", \"en\": \"üá¨üáß\", \n        \"pl\": \"üáµüá±\", \"fr\": \"üá´üá∑\", \"it\": \"üáÆüáπ\",\n        \"lt\": \"üá±üáπ\"\n    }\n    return flags.get(lang_code, \"üåê\")\n\ndef parse_processed_md(filepath):\n    \"\"\"\n    –ß–∏—Ç–∞–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π MD-—Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n    - meta: —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–Ω–∏–≥–∏\n    - clean_text: –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –±–µ–∑ APPX-—Ä–∞–∑–¥–µ–ª–∞\n    - page_media: —Å–ª–æ–≤–∞—Ä—å {file_id(int) -> [{{\"id\",\"caption\",\"link\"}}]} –¥–ª—è –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π –∏ –ø—Ä–æ—á–∏—Ö –º–µ–¥–∏–∞\n    - page_ocr: —Å–ª–æ–≤–∞—Ä—å {file_id(int) -> [{{\"id\",\"caption\",\"link\"}}]} –¥–ª—è OCR JSON-—Ñ–∞–π–ª–æ–≤\n    - page_scans: —Å–ª–æ–≤–∞—Ä—å {file_id(int) -> tg_msg_id(int)} –¥–ª—è –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å–∫–∞–Ω–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü (Ref)\n    \"\"\"\n    if not os.path.exists(filepath):\n        return None, None, {}, {}, {}, {}\n\n    with open(filepath, 'r', encoding='utf-8') as f:\n        content = f.read()\n\n    # --- 1. Metadata ---\n    meta = {}\n    meta_match = re.search(r'## Metadata\\n(.*?)\\n\\n## Content', content, re.DOTALL)\n    if meta_match:\n        lines = meta_match.group(1).split('\\n')\n        for line in lines:\n            if line.strip().startswith('- **'):\n                match_row = re.search(r'\\*\\*(\\w+)\\*\\*: (.*)', line)\n                if match_row:\n                    key = match_row.group(1)\n                    val = match_row.group(2).strip()\n                    if val.lower() != \"null\" and val != \"\":\n                        meta[key] = val\n\n    # --- 2. Main text (without APPX) ---\n    text_match = re.search(r'## Content\\n(.*?)(\\n## APPX|$)', content, re.DOTALL)\n    clean_text = text_match.group(1).strip() if text_match else \"\"\n\n    # --- 3. APPX: Resources -> page_media + page_ocr ---\n    page_media = {}\n    page_ocr = {}\n    appx_match = re.search(r'## APPX: Resources\\n(.*)', content, re.DOTALL)\n    if appx_match:\n        lines = appx_match.group(1).split('\\n')\n        current = None\n        for line in lines:\n            line = line.rstrip()\n            if not line:\n                continue\n\n            m_id = re.match(r\"- \\*\\*ID\\*\\*: `([^`]+)`\", line)\n            if m_id:\n                # –§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –∑–∞–ø–∏—Å—å\n                if current and current.get(\"id\"):\n                    mid = current[\"id\"]\n                    m_page = re.search(r'_(\\d+)(?:_|$)', mid)\n                    if m_page:\n                        try:\n                            page = int(m_page.group(1))\n                        except ValueError:\n                            page = None\n                        if page is not None:\n                            # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ media/ocr –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç id\n                            if mid.lower().startswith(\"ocr_\"):\n                                page_ocr.setdefault(page, []).append(current)\n                            else:\n                                page_media.setdefault(page, []).append(current)\n                current = {\"id\": m_id.group(1)}\n                continue\n\n            m_cap = re.match(r\"- \\*\\*Caption\\*\\*: (.*)\", line)\n            if m_cap and current is not None:\n                current[\"caption\"] = m_cap.group(1).strip()\n                continue\n\n            m_link = re.match(r\"- \\*\\*Link\\*\\*: (.*)\", line)\n            if m_link and current is not None:\n                current[\"link\"] = m_link.group(1).strip()\n                # –§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ Link\n                mid = current.get(\"id\")\n                if mid:\n                    m_page = re.search(r'_(\\d+)(?:_|$)', mid)\n                    if m_page:\n                        try:\n                            page = int(m_page.group(1))\n                        except ValueError:\n                            page = None\n                        if page is not None:\n                            if mid.lower().startswith(\"ocr_\"):\n                                page_ocr.setdefault(page, []).append(current)\n                            else:\n                                page_media.setdefault(page, []).append(current)\n                current = None\n\n        # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π —Ñ–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å –±–µ–∑ Link\n        if current and current.get(\"id\"):\n            mid = current[\"id\"]\n            m_page = re.search(r'_(\\d+)(?:_|$)', mid)\n            if m_page:\n                try:\n                    page = int(m_page.group(1))\n                except ValueError:\n                    page = None\n                if page is not None:\n                    if mid.lower().startswith(\"ocr_\"):\n                        page_ocr.setdefault(page, []).append(current)\n                    else:\n                        page_media.setdefault(page, []).append(current)\n\n    # --- 4. Page scans (File: N | Ref: tg_id, OCRjson: ids) ---\n    page_scans = {}\n    page_ocr_refs = {}\n    # –ò—â–µ–º –º–∞—Ä–∫–µ—Ä—ã –≤–∏–¥–∞ [File: 3 | Book: ... | Ref: 12345] –∏–ª–∏ [File: 3 | Ref: 12345]\n    # –¢–µ–ø–µ—Ä—å –º–∞—Ä–∫–µ—Ä –º–æ–∂–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ö–≤–æ—Å—Ç –≤–∏–¥–∞ \"| OCRjson: 1415,1416\"\n    pattern_scans = r'\\[File:\\s*(\\d+)(?:\\s*\\|\\s*Book:[^\\]|]*)?\\s*\\|\\s*Ref:\\s*(\\d+)(?:\\s*\\|\\s*OCRjson:\\s*([\\d,\\s]+))?\\]'\n    for m in re.finditer(pattern_scans, content):\n        try:\n            fid = int(m.group(1))\n            tg_id = int(m.group(2))\n            page_scans[fid] = tg_id\n\n            ocr_raw = m.group(3)\n            if ocr_raw:\n                ids = []\n                for part in re.split(r'[,\\s]+', ocr_raw.strip()):\n                    if not part:\n                        continue\n                    try:\n                        ids.append(int(part))\n                    except ValueError:\n                        continue\n                if ids:\n                    existing = page_ocr_refs.get(fid, [])\n                    for vid in ids:\n                        if vid not in existing:\n                            existing.append(vid)\n                    page_ocr_refs[fid] = existing\n        except ValueError:\n            continue\n\n    return meta, clean_text, page_media, page_ocr, page_scans, page_ocr_refs\n\n\n\ndef create_windows_with_overlap(text, window_size, overlap):\n    \"\"\"\n    –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –Ω–∞—Ä–µ–∑–∫–∞ –æ–∫–æ–Ω —Å –Ω–∞—Ö–ª–µ—Å—Ç–æ–º.\n    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö —Ü–∏–∫–ª–æ–≤.\n    \"\"\"\n    paragraphs = text.split('\\n\\n')\n    windows = []\n    \n    current_window = []\n    current_len = 0\n    \n    for para in paragraphs:\n        para_len = len(para)\n        \n        # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏–º–∏—Ç –æ–∫–Ω–∞\n        if current_len + para_len > window_size:\n            # 1. –ï—Å–ª–∏ –≤ –æ–∫–Ω–µ —É–∂–µ —á—Ç–æ-—Ç–æ –µ—Å—Ç—å, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –∫–∞–∫ –≥–æ—Ç–æ–≤–æ–µ –æ–∫–Ω–æ\n            if current_window:\n                windows.append(\"\\n\\n\".join(current_window))\n                \n                # 2. –í—ã—á–∏—Å–ª—è–µ–º Overlap (—Ö–≤–æ—Å—Ç —Ç–µ–∫—É—â–µ–≥–æ –æ–∫–Ω–∞ —Å—Ç–∞–Ω–µ—Ç –≥–æ–ª–æ–≤–æ–π —Å–ª–µ–¥—É—é—â–µ–≥–æ)\n                # –ò–¥–µ–º –Ω–∞–∑–∞–¥, –ø–æ–∫–∞ –Ω–µ –Ω–∞–±–µ—Ä–µ–º –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤\n                back_buffer = []\n                back_len = 0\n                for p in reversed(current_window):\n                    if back_len >= overlap: break\n                    back_buffer.insert(0, p)\n                    back_len += len(p)\n                \n                # 3. –ù–æ–≤–æ–µ –æ–∫–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å Overlap\n                current_window = back_buffer\n                current_len = back_len\n        \n        # 4. –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ (–±–µ–∑—É—Å–ª–æ–≤–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º, —á—Ç–æ–±—ã –¥–≤–∏–≥–∞—Ç—å—Å—è –≤–ø–µ—Ä–µ–¥)\n        current_window.append(para)\n        current_len += para_len\n        \n    # –ù–µ –∑–∞–±—ã–≤–∞–µ–º —Ö–≤–æ—Å—Ç\n    if current_window:\n        windows.append(\"\\n\\n\".join(current_window))\n        \n    return windows\n\ndef get_chunking_prompt(book_meta, text_window, prev_context):\n    title = book_meta.get('title', 'Unknown Title')\n\n    # Where in the document we are (for locators)\n    loc_str = \"Start of document.\"\n    if prev_context.get(\"file_id\", 1) != 1 or prev_context.get(\"tg_msg_id\", 0) != 0:\n        loc_str = f\"[File: {prev_context.get('file_id', 1)} | Book: {prev_context.get('book_page', '1')} | Ref: {prev_context.get('tg_msg_id', 0)}]\"\n\n    # Short context from the last accepted content chunk\n    last_text_context = \"No previous text.\"\n    if prev_context.get(\"last_chunk_text\"):\n        safe_text = prev_context['last_chunk_text'].replace('\"', \"'\")\n        last_text_context = f\"\\\"{safe_text}\\\"\"\n\n    return f\"\"\"\n    ROLE: Database Engineer and RAG chunker.\n    TASK: Split text into SMALL, COHERENT SEMANTIC CHUNKS (for vector search).\n\n    DOCUMENT INFO:\n    Title: \"{title}\"\n\n    CONTEXT FROM PREVIOUS BATCH:\n    - Last Location: {loc_str}\n    - LAST CHUNK CONTENT (use this to fix broken sentences):\n      {last_text_context}\n\n    INPUT TEXT (contains ~{WINDOW_OVERLAP} chars overlap from previous batch):\n    \\\"\"\"{text_window}\\\"\"\" \n\n    INSTRUCTIONS:\n    1. ENTITY TYPES:\n       - Use `chunk_type` = \"content\" for the main narrative, analysis, introductions, conclusions, and abstracts (–¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º).\n       - Use `chunk_type` = \"bibliography\" for lists of literature, references and source lists.\n       - Use `chunk_type` = \"footnotes_definitions\" for footnotes, archival references, –ø–æ—è—Å–Ω–µ–Ω–∏—è –∫ —Å—Å—ã–ª–∫–∞–º.\n\n    2. CHUNK SIZE & SENTENCE INTEGRITY:\n       - TARGET SIZE: Aim for 500‚Äì900 characters per `chunk_content`.\n       - HARD LIMIT: Each `chunk_content` MUST be < {MAX_CHARS_PER_CHUNK} characters.\n       - NEVER cut a sentence between chunks. Each `chunk_content` should end at a sentence boundary (. ? ! ‚Ä¶ or closing quotes), except for very short headings.\n       - If the previous `LAST CHUNK CONTENT` ended in the middle of a sentence, use the overlapped text to restore the FULL sentence inside a single chunk.\n       - If you must choose between slightly exceeding the target and breaking a sentence, prefer a slightly longer chunk (but still under the HARD LIMIT).\n\n    3. OVERLAP & CONTINUITY:\n       - The `INPUT TEXT` starts with text from the previous batch (overlap of about {WINDOW_OVERLAP} characters).\n       - DO NOT DUPLICATE: do not output text that is largely identical to `LAST CHUNK CONTENT`.\n       - The first chunk in this batch should smoothly continue after `LAST CHUNK CONTENT`, without repeating it.\n\n    4. LOCATOR EXTRACTION:\n       - For every chunk, find the NEAREST PRECEDING marker `> [File: X | Book: Y | Ref: Z]` in the original text.\n       - If no marker is present inside this batch (because it is only in the overlap), USE the `Last Location` from the context instead.\n       - Fill `file_id`, `book_page`, `tg_msg_id` based on this nearest marker / context.\n       - If a chunk clearly spans TWO neighbouring pages (text continues after the next marker),\n         also fill `file_id_2`, `book_page_2`, `tg_msg_id_2` for the second page; otherwise set them to null.\n\n    5. LANGUAGE & METADATA:\n       - Field `lang`: two-letter language code such as \"ru\", \"de\", \"en\", \"pl\", etc.\n       - Fields `semantic_header` and `keywords` MUST be written in Russian, even if `chunk_content` is in another language.\n       - EXCEPTION: if `lang` == \"de\" (German text), write `semantic_header` and `keywords` in German.\n       - For English abstracts, summaries and keyword blocks that describe a Russian article, STILL produce a Russian `semantic_header` and Russian `keywords` that accurately convey the meaning.\n       - `semantic_header` should be 1 short sentence or phrase summarising the chunk.\n       - `keywords` should be a short list (3‚Äì8) of topical tags in Russian.\n\n    6. RELEVANCE SCORE (0.0‚Äì1.0):\n       - 0.9‚Äì1.0: Core analytical or narrative content that is central to the document‚Äôs main topic.\n       - 0.6‚Äì0.8: Important supporting text: introductions, abstracts (including foreign-language abstracts), conclusions, author commentary.\n       - 0.3‚Äì0.5: Contextual but secondary material (less central details, local digressions).\n       - 0.1‚Äì0.2: Mostly technical or marginal text: isolated archival references, very noisy OCR, marginalia.\n       - 0.0: Only for garbage (pure OCR noise, meaningless fragments, empty chunks).\n       - Abstracts, summaries and keyword blocks are NEVER garbage: do NOT assign them 0.0 unless the text is unreadable.\n       - Bibliography and footnotes can have lower scores (e.g. 0.2‚Äì0.5), but should still honestly reflect their usefulness.\n\n    7. PERSONS & LOCATIONS (for metadata filters):\n    7. PERSONS & LOCATIONS (for metadata filters):\n    - For each chunk, also extract two additional fields:\n      - `persons`: a list of real people (full names where possible ‚Äî philosophers, rulers, clergy, authors, etc.) explicitly mentioned in `chunk_content`. Do NOT include generic roles like \"king\" or \"author\" without names.\n      - `locations`: a list of geographic or topographical entities (cities, towns, villages, regions, rivers, churches, monasteries, streets, districts, buildings, landmarks) explicitly mentioned in `chunk_content` or clearly and unambiguously implied by it (for example, \"–ö–∞—Ñ–µ–¥—Ä–∞–ª—å–Ω—ã–π —Å–æ–±–æ—Ä\" in K√∂nigsberg).\n    - If there are no persons or locations, return empty arrays: `\"persons\": []`, `\"locations\": []` (do NOT use null).\n    - PRIORITIZE MICRO-LOCATIONS: when possible, focus on specific landmarks, buildings, streets, quarters and districts (e.g., \"–ë–∞—à–Ω—è –î–æ–Ω–∞\", \"–û–∑–µ—Ä–æ –í–µ—Ä—Ö–Ω–µ–µ\", \"Hotel Sambia\", \"–î–æ–º –°–∫–∞–∑–æ—á–Ω–∏–∫–∞\") instead of only broad city names.\n    - INFER TOWN FROM LANDMARK (Kaliningrad region): if a landmark unambiguously belongs to a town in the Kaliningrad region, you may include both the object itself and the town name as separate location entries. Example: \"Hotel Sambia\" ‚Üí locations: [\"Hotel Sambia\", \"–ó–µ–ª–µ–Ω–æ–≥—Ä–∞–¥—Å–∫\"]; \"–î–æ–º –°–∫–∞–∑–æ—á–Ω–∏–∫–∞\" ‚Üí locations: [\"–î–æ–º –°–∫–∞–∑–æ—á–Ω–∏–∫–∞\", \"–°–≤–µ—Ç–ª–æ–≥–æ—Ä—Å–∫\"].\n    - AVOID OVER-GENERIC KALININGRAD: do not add \"–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥\" or \"–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å\" to `locations` if more specific locations (streets, buildings, landmarks) inside the city or region are already listed. Keep \"–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥\" only if the text discusses the city as a whole (e.g., its history, administration, population) rather than a particular point of interest.\n\n    8. LANGUAGE & ORTHOGRAPHY (CRITICAL):\n       - Preserve the original spelling in `chunk_content` (keep '—£', '—ä', 'i' etc.). Do NOT modernize the visible text.\n       - NORMALIZATION FOR METADATA: All lists (`keywords`, `persons`, `locations`) MUST be converted to Modern Russian spelling.\n         * Example: text says \"–ö–µ–Ω–∏–≥—Å–±–µ—Ä–≥—£\" ‚Üí location tag must be \"–ö–µ–Ω–∏–≥—Å–±–µ—Ä–≥\".\n         * Example: text says \"–§–æ—Ä–¥–µ—Ä–µ –§–æ—Ä—à—Ç–∞–¥—Ç—ä\" ‚Üí location tag must be \"–§–æ—Ä–¥–µ—Ä–µ –§–æ—Ä—à—Ç–∞–¥—Ç\".\n       - NORMALIZATION FOR SEARCH: Populate the `normalized_content` field with the full chunk text converted to modern Russian spelling.\n       - If the text uses pre-1918 orthography, set `\"orthography\": \"pre_reform\"`. Otherwise use `\"orthography\": \"modern\"`.\n\n\n    \n    OUTPUT JSON FORMAT (Array):\n    [\n      {{\n        \"chunk_type\": \"content\" | \"bibliography\" | \"footnotes_definitions\",\n        \"chunk_content\": \"string (ORIGINAL text with —£, —ä, i - preserve exactly as in source)\",\n        \"normalized_content\": \"string (Modern Russian spelling for search indexing)\",\n        \"file_id\": int,\n        \"book_page\": \"string\",\n        \"tg_msg_id\": int,\n        \"file_id_2\": int | null,\n        \"book_page_2\": \"string\" | null,\n        \"tg_msg_id_2\": int | null,\n        \"relevance_score\": float,\n        \"lang\": \"code\",\n        \"semantic_header\": \"string\",\n        \"keywords\": [\"tag1\", \"tag2\", \"tag3\"],\n        \"persons\": [\"–ò–º–º–∞–Ω—É–∏–ª –ö–∞–Ω—Ç\", \"–ì–µ—Ä—Ü–æ–≥ –ê–ª—å–±—Ä–µ—Ö—Ç\"],\n        \"locations\": [\"–ö—ë–Ω–∏–≥—Å–±–µ—Ä–≥\", \"–ö–∞—Ñ–µ–¥—Ä–∞–ª—å–Ω—ã–π —Å–æ–±–æ—Ä\"],\n        \"orthography\": \"modern\" | \"pre_reform\"\n      }}\n    ]\n    \"\"\"\n\n# === SUPABASE / POSTGRES RAG STORAGE ===\ndef db_log(msg: str):\n    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"[DB] {ts} {msg}\")\n\ndef get_db_url():\n    url = os.getenv(\"SUPABASE_DB_URL\")\n    if not url:\n        try:\n            user_secrets = UserSecretsClient()\n            url = user_secrets.get_secret(\"SUPABASE_DB_URL\")\n        except Exception:\n            url = None\n    if not url:\n        raise RuntimeError(\"SUPABASE_DB_URL not found. Set SUPABASE_DB_URL env or Kaggle secret.\")\n    return url\n\ndef ensure_db_schema():\n    \"\"\"\n    –ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ —Å–æ–∑–¥–∞—ë—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∏ —Ç–∞–±–ª–∏—Ü—ã:\n    books, media, chunks (–±–µ–∑ —Ç–∞–±–ª–∏—Ü—ã pages).\n    \"\"\"\n    db_url = get_db_url()\n    db_log(\"Connecting to database for schema check...\")\n    conn = psycopg2.connect(db_url)\n    conn.autocommit = True\n    cur = conn.cursor()\n\n    # Extensions\n    db_log(\"Checking extensions (vector, pg_trgm, pgcrypto)...\")\n    cur.execute(\"create extension if not exists vector;\")\n    cur.execute(\"create extension if not exists pg_trgm;\")\n    cur.execute(\"create extension if not exists pgcrypto;\")\n\n    # books\n    db_log(\"Checking table books...\")\n    cur.execute(\"select to_regclass('public.books');\")\n    if cur.fetchone()[0] is None:\n        db_log(\"Creating table books...\")\n        cur.execute(\"\"\"create table public.books (\n            id uuid primary key default gen_random_uuid(),\n            isbn text unique,\n            title text not null,\n            subtitle text,\n            authors text,\n            co_authors text,\n            publisher text,\n            year int,\n            document_type text,\n            historical_period_description text,\n            historical_year_start int,\n            historical_year_end int,\n            summary text,\n            source_telegram_channel_id bigint,\n            source_origin_msg_id bigint,\n            source_origin_file text,\n            meta jsonb,\n            created_at timestamptz default now(),\n            updated_at timestamptz default now()\n        );\"\"\")\n        cur.execute(\"create index idx_books_title_trgm on public.books using gin (title gin_trgm_ops);\")\n        cur.execute(\"create index idx_books_year on public.books(year);\")\n        db_log(\"Table books created.\")\n    else:\n        db_log(\"Table books already exists.\")\n\n    # media\n    db_log(\"Checking table media...\")\n    cur.execute(\"select to_regclass('public.media');\")\n    if cur.fetchone()[0] is None:\n        db_log(\"Creating table media...\")\n        cur.execute(\"\"\"create table public.media (\n            id bigserial primary key,\n            book_id uuid references public.books(id) on delete cascade,\n            internal_id text,\n            tg_msg_id bigint,\n            file_id int,\n            book_page text,\n            caption text,\n            meta jsonb,\n            created_at timestamptz default now()\n        );\"\"\")\n        cur.execute(\"create index idx_media_book on public.media(book_id);\")\n        db_log(\"Table media created.\")\n    else:\n        db_log(\"Table media already exists.\")\n\n    # chunks\n    db_log(\"Checking table chunks...\")\n    cur.execute(\"select to_regclass('public.chunks');\")\n    if cur.fetchone()[0] is None:\n        db_log(\"Creating table chunks...\")\n        cur.execute(\"\"\"create table public.chunks (\n            id bigserial primary key,\n            book_id uuid references public.books(id) on delete cascade,\n            file_id int,\n            book_page text,\n            tg_msg_id bigint,\n            position int,\n            chunk_type text,\n            lang text,\n            relevance_score real,\n            topic text,\n            keywords text[],\n            persons text[],\n            locations text[],\n            orthography text,\n            chunk_year_start int,\n            chunk_year_end int,\n            scan_page_ids int[],\n            scan_tg_msg_ids bigint[],\n            ocr_tg_msg_ids bigint[],\n            media_ids bigint[],\n            media_internal_ids text[],\n            content text,\n            embedding vector(768),\n            created_at timestamptz default now()\n        );\"\"\")\n        cur.execute(\"create index idx_chunks_book on public.chunks(book_id);\")\n        cur.execute(\"create index idx_chunks_relevance on public.chunks(relevance_score);\")\n        cur.execute(\"create index idx_chunks_embedding on public.chunks using hnsw (embedding vector_cosine_ops);\")\n        db_log(\"Table chunks created.\")\n    else:\n        db_log(\"Table chunks already exists.\")\n        cur.execute(\"alter table public.chunks add column if not exists persons text[];\")\n        cur.execute(\"alter table public.chunks add column if not exists locations text[];\")\n        cur.execute(\"alter table public.chunks add column if not exists orthography text;\")\n        cur.execute(\"alter table public.chunks add column if not exists scan_page_ids int[];\")\n        cur.execute(\"alter table public.chunks add column if not exists scan_tg_msg_ids bigint[];\")\n        cur.execute(\"alter table public.chunks add column if not exists ocr_tg_msg_ids bigint[];\")\n\n\n        # === CREATE / UPDATE VECTOR SEARCH FUNCTION ===\n    db_log(\"Checking/Updating match_chunks function...\")\n\n    # 1. –î–û–ë–ê–í–ò–¢–¨ –≠–¢–£ –°–¢–†–û–ö–£ (—É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏):\n    cur.execute(\"drop function if exists match_chunks(vector(768), float, int);\")\n\n    cur.execute(\"\"\"\n    create or replace function match_chunks (\n      query_embedding vector(768),\n      match_threshold float,\n      match_count int\n    )\n    returns table (\n      -- (—Ç—É—Ç –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å–ø–∏—Å–æ–∫ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –ø–æ–ª–µ–π)\n      id int8,\n      book_id uuid,\n      content text,\n      similarity float,\n      chunk_type text,\n      lang text,\n      relevance_score real,\n      topic text,\n      keywords text[],\n      persons text[],\n      locations text[],\n      orthography text,\n      chunk_year_start int,\n      chunk_year_end int,\n      scan_page_ids int[],\n      scan_tg_msg_ids bigint[],\n      ocr_tg_msg_ids bigint[],\n      media_ids bigint[],\n      media_internal_ids text[],\n      tg_msg_id bigint,\n      book_page text,\n      book_title text,\n      book_authors text,\n      book_year int,\n      book_isbn text,\n      source_links text[]\n    )\n    language plpgsql\n    as $$\n    begin\n      return query\n      select\n        c.id,\n        c.book_id,\n        c.content,\n        1 - (c.embedding <=> query_embedding) as similarity,\n        c.chunk_type,\n        c.lang,\n        c.relevance_score,\n        c.topic,\n        c.keywords,\n        c.persons,\n        c.locations,\n        c.orthography,\n        c.chunk_year_start,\n        c.chunk_year_end,\n        c.scan_page_ids,\n        c.scan_tg_msg_ids,\n        c.ocr_tg_msg_ids,\n        c.media_ids,\n        c.media_internal_ids,\n        c.tg_msg_id,\n        c.book_page,\n        b.title as book_title,\n        b.authors as book_authors,\n        b.year as book_year,\n        b.isbn as book_isbn,\n        case\n          when c.scan_tg_msg_ids is null or array_length(c.scan_tg_msg_ids, 1) = 0 then\n            array[\n              'https://t.me/c/' || b.source_telegram_channel_id::text || '/' || c.tg_msg_id::text\n            ]\n          else (\n            -- 2. –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ó–î–ï–°–¨ (scan_id –≤–º–µ—Å—Ç–æ id):\n            select array_agg(\n              'https://t.me/c/' || b.source_telegram_channel_id::text || '/' || scan_id::text\n            )\n            from unnest(c.scan_tg_msg_ids) as scan_id\n          )\n        end as source_links\n      from chunks c\n      left join books b on c.book_id = b.id\n      where 1 - (c.embedding <=> query_embedding) > match_threshold\n      order by c.embedding <=> query_embedding\n      limit match_count;\n    end;\n    $$;\n    \"\"\")\n\n    cur.close()\n    conn.close()\n    db_log(\"Schema check complete.\")\n\ndef find_existing_book(conn, meta: dict):\n    \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç id –∫–Ω–∏–≥–∏, –µ—Å–ª–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ books —É–∂–µ –µ—Å—Ç—å –∑–∞–ø–∏—Å—å —Å —Ç–∞–∫–∏–º –∂–µ ISBN\n    –∏–ª–∏ (title + year). –ù–∏—á–µ–≥–æ –Ω–µ —É–¥–∞–ª—è–µ—Ç –∏ –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç.\"\"\"\n    cur = conn.cursor()\n    isbn = (meta.get(\"isbn\") or \"\").strip() or None\n    title = (meta.get(\"title\") or \"\").strip()\n    year = meta.get(\"year\")\n\n    if isbn:\n        cur.execute(\"select id from public.books where isbn = %s;\", (isbn,))\n        row = cur.fetchone()\n        if row:\n            db_log(f\"Book with same ISBN already exists: {isbn}\")\n            return row[0]\n\n    if title and year:\n        cur.execute(\"select id from public.books where title = %s and year = %s limit 1;\", (title, year))\n        row = cur.fetchone()\n        if row:\n            db_log(f\"Book with same title+year already exists: {title} ({year})\")\n            return row[0]\n\n    return None\n\ndef insert_book_and_related(book_data: dict):\n    \"\"\"–í—Å—Ç–∞–≤–ª—è–µ—Ç –æ–¥–Ω—É –∫–Ω–∏–≥—É –∏ –≤—Å–µ –µ—ë —á–∞–Ω–∫–∏/–º–µ–¥–∏–∞ –≤ –ë–î Supabase.\n    –ù–ò–ß–ï–ì–û –Ω–µ —É–¥–∞–ª—è–µ—Ç –∏ –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç, –µ—Å–ª–∏ –∫–Ω–∏–≥–∞ —É–∂–µ –µ—Å—Ç—å (isbn –∏–ª–∏ title+year).\n    –¢–∞–∫–∂–µ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –ø—Ä–∏–≤–æ–¥–∏—Ç media_ids –∫ bigint[], –∏–≥–Ω–æ—Ä–∏—Ä—É—è –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.\n    \"\"\"\n    db_url = get_db_url()\n    ensure_db_schema()\n    conn = psycopg2.connect(db_url)\n    conn.autocommit = False\n    cur = conn.cursor()\n\n    meta = book_data.get(\"metadata\", {})\n    page_ocr_map = book_data.get(\"page_ocr\") or {}\n\n    def extract_tg_msg_id_from_link(link: str):\n        \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ–≤–æ–π msg_id –∏–∑ —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞ 'tg://msg_id?id=1007'.\"\"\"\n        if not link:\n            return None\n        m = re.search(r\"id=(\\d+)\", link)\n        if not m:\n            return None\n        try:\n            return int(m.group(1))\n        except ValueError:\n            return None\n\n\n    existing_id = find_existing_book(conn, meta)\n    if existing_id:\n        db_log(f\"Skip insert: book already exists in DB (id={existing_id}).\")\n        conn.close()\n        return\n\n    db_log(\"Inserting new book row...\")\n    cur.execute(\n        \"\"\"insert into public.books(\n            isbn, title, subtitle, authors, co_authors, publisher, year,\n            document_type, historical_period_description,\n            historical_year_start, historical_year_end,\n            summary, source_telegram_channel_id,\n            source_origin_msg_id, source_origin_file, meta\n        ) values (\n            %(isbn)s, %(title)s, %(subtitle)s, %(authors)s, %(co_authors)s, %(publisher)s, %(year)s,\n            %(document_type)s, %(historical_period_description)s,\n            %(historical_year_start)s, %(historical_year_end)s,\n            %(summary)s, %(source_telegram_channel_id)s,\n            %(source_origin_msg_id)s, %(source_origin_file)s, %(meta)s\n        ) returning id;\"\"\",\n        {\n            \"isbn\": meta.get(\"isbn\"),\n            \"title\": meta.get(\"title\"),\n            \"subtitle\": meta.get(\"subtitle\"),\n            \"authors\": meta.get(\"authors\"),\n            \"co_authors\": meta.get(\"co_authors\"),\n            \"publisher\": meta.get(\"publisher\"),\n            \"year\": meta.get(\"year\"),\n            \"document_type\": meta.get(\"document_type\"),\n            \"historical_period_description\": meta.get(\"historical_period_description\"),\n            \"historical_year_start\": meta.get(\"historical_year_start\"),\n            \"historical_year_end\": meta.get(\"historical_year_end\"),\n            \"summary\": meta.get(\"summary\"),\n            \"source_telegram_channel_id\": meta.get(\"source_telegram_channel_id\"),\n            \"source_origin_msg_id\": meta.get(\"source_origin_msg_id\"),\n            \"source_origin_file\": meta.get(\"source_origin_file\"),\n            \"meta\": json.dumps(meta, ensure_ascii=False),\n        },\n    )\n    book_id = cur.fetchone()[0]\n    db_log(f\"Book inserted with id={book_id}\")\n\n    # MEDIA: –±–µ—Ä—ë–º page_media –∫–∞–∫ –µ—Å—Ç—å\n    page_media = book_data.get(\"page_media\") or {}\n    media_rows = []\n    for page_key, media_list in page_media.items():\n        try:\n            file_id = int(page_key)\n        except (TypeError, ValueError):\n            file_id = None\n        for m in media_list:\n            internal_id = m.get(\"id\")\n            caption = m.get(\"caption\")\n            link = m.get(\"link\") or \"\"\n            tg_msg_id = None\n            m_id = re.search(r\"id=(\\d+)\", link)\n            if m_id:\n                try:\n                    tg_msg_id = int(m_id.group(1))\n                except ValueError:\n                    tg_msg_id = None\n            media_rows.append(\n                (\n                    book_id,\n                    internal_id,\n                    tg_msg_id,\n                    file_id,\n                    None,\n                    caption,\n                    json.dumps(m, ensure_ascii=False),\n                )\n            )\n\n    if media_rows:\n        db_log(f\"Inserting {len(media_rows)} media rows...\")\n        execute_values(\n            cur,\n            \"insert into public.media (book_id, internal_id, tg_msg_id, file_id, book_page, caption, meta) values %s\",\n            media_rows,\n        )\n\n    # CHUNKS\n    all_chunks = []\n    for section_name in (\"chunks\", \"bibliography\", \"footnotes\"):\n        for ch in book_data.get(section_name, []):\n            all_chunks.append(ch)\n\n    def embed_one(text: str):\n        \"\"\"–°—Ç—Ä–æ–∏—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –ü—Ä–∏ –æ—à–∏–±–∫–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None.\"\"\"\n        try:\n            if not text:\n                text = \"\"\n            result = genai.embed_content(model=\"text-embedding-004\", content=text)\n            if isinstance(result, dict) and \"embedding\" in result:\n                return result[\"embedding\"]\n            return result\n        except Exception as e:\n            db_log(f\"Embedding error: {e}\")\n            return None\n\n    chunk_rows = []\n    for pos, ch in enumerate(all_chunks):\n        original_text = ch.get(\"chunk_content\") or \"\"\n        normalized_text = ch.get(\"normalized_content\") or original_text\n        text = normalized_text\n\n        # prepare metadata for enriched embedding\n        keywords = ch.get(\"keywords\") or []\n        if isinstance(keywords, str):\n            keywords = [k.strip() for k in keywords.split(\",\") if k.strip()]\n\n        persons = ch.get(\"persons\") or []\n        if isinstance(persons, str):\n            persons = [p.strip() for p in persons.split(\",\") if p.strip()]\n\n        locations = ch.get(\"locations\") or []\n        if isinstance(locations, str):\n            locations = [l.strip() for l in locations.split(\",\") if l.strip()]\n\n        orthography = ch.get(\"orthography\") or None\n\n        semantic_header = ch.get(\"semantic_header\") or \"\"\n\n        rich_parts = []\n        if semantic_header:\n            rich_parts.append(f\"–¢–µ–º–∞: {semantic_header}\")\n        if keywords:\n            rich_parts.append(\"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: \" + \", \".join(keywords))\n        if persons:\n            rich_parts.append(\"–ü–µ—Ä—Å–æ–Ω—ã: \" + \", \".join(persons))\n        if locations:\n            rich_parts.append(\"–õ–æ–∫–∞—Ü–∏–∏: \" + \", \".join(locations))\n        if text:\n            rich_parts.append(\"–¢–µ–∫—Å—Ç: \" + text)\n        rich_context = \"\\n\".join(rich_parts)\n\n        emb = embed_one(rich_context)\n\n        # file_id / book_page / tg_msg_id\n        try:\n            fid_int = int(ch.get(\"file_id\")) if ch.get(\"file_id\") is not None else None\n        except (TypeError, ValueError):\n            fid_int = None\n\n        book_page = str(ch.get(\"book_page\")) if ch.get(\"book_page\") is not None else None\n        tg_msg_id = ch.get(\"tg_msg_id\")\n\n        # keywords\n        keywords = ch.get(\"keywords\") or []\n        if isinstance(keywords, str):\n            keywords = [k.strip() for k in keywords.split(\",\") if k.strip()]\n\n        # media_ids: –ø—Ä–∏–≤–æ–¥–∏–º –∫ —Å–ø–∏—Å–∫—É bigint, –∞ –≤—Å–µ –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –¥–æ–±–∞–≤–ª—è–µ–º –∫ media_internal_ids\n        raw_media_ids = ch.get(\"media_ids\") or []\n        if not isinstance(raw_media_ids, (list, tuple)):\n            raw_media_ids = [raw_media_ids]\n\n        cast_media_ids = []\n        extra_internal_ids = []\n        for x in raw_media_ids:\n            if x is None:\n                continue\n            # —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∫–∞–∫ —á–∏—Å–ª–æ\n            if isinstance(x, (int, float)):\n                cast_media_ids.append(int(x))\n                continue\n            s = str(x).strip()\n            if not s:\n                continue\n            try:\n                cast_media_ids.append(int(s))\n            except ValueError:\n                extra_internal_ids.append(s)\n\n        # media_internal_ids –∏—Å—Ö–æ–¥–Ω—ã–µ + –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –∏–∑ \"–±–∏—Ç—ã—Ö\" media_ids\n        internal_ids = ch.get(\"media_internal_ids\") or []\n        if not isinstance(internal_ids, (list, tuple)):\n            internal_ids = [internal_ids]\n        internal_ids = list(internal_ids) + extra_internal_ids\n\n        # –ü—Ä–∏–≤–æ–¥–∏–º scan_page_ids –∏ scan_tg_msg_ids –∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ–º—É –≤–∏–¥—É\n        scan_page_ids = ch.get(\"scan_page_ids\") or None\n        if isinstance(scan_page_ids, int):\n            scan_page_ids = [scan_page_ids]\n        if isinstance(scan_page_ids, (list, tuple)):\n            scan_page_ids = [int(p) for p in scan_page_ids if p is not None]\n            if not scan_page_ids:\n                scan_page_ids = None\n\n        scan_tg_ids = ch.get(\"scan_tg_msg_ids\") or None\n        if isinstance(scan_tg_ids, (int, float)):\n            scan_tg_ids = [int(scan_tg_ids)]\n        if isinstance(scan_tg_ids, (list, tuple)):\n            scan_tg_ids = [int(t) for t in scan_tg_ids if t is not None]\n            if not scan_tg_ids:\n                scan_tg_ids = None\n\n        # OCR JSON ids: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ, —á—Ç–æ —É–∂–µ –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–æ –∫ —á–∞–Ω–∫—É,\n        # –∞ –µ—Å–ª–∏ –ø—É—Å—Ç–æ ‚Äî –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ page_ocr_map –∏ scan_page_ids\n        ocr_ids = ch.get(\"ocr_tg_msg_ids\") or None\n        if (not ocr_ids) and scan_page_ids and page_ocr_map:\n            tmp = []\n            for pid in scan_page_ids:\n                page_ocr_list = page_ocr_map.get(pid) or []\n                for mobj in page_ocr_list:\n                    msg_id = extract_tg_msg_id_from_link(mobj.get(\"link\", \"\"))\n                    if msg_id is not None and msg_id not in tmp:\n                        tmp.append(msg_id)\n            if tmp:\n                ocr_ids = tmp\n\n        if isinstance(ocr_ids, (int, float)):\n            ocr_ids = [int(ocr_ids)]\n        if isinstance(ocr_ids, (list, tuple)):\n            ocr_ids = [int(t) for t in ocr_ids if t is not None]\n            if not ocr_ids:\n                ocr_ids = None\n\n        chunk_rows.append(\n            (\n                book_id,\n                fid_int,\n                book_page,\n                tg_msg_id,\n                pos,\n                ch.get(\"chunk_type\") or \"content\",\n                ch.get(\"lang\"),\n                float(ch.get(\"relevance_score\") or 0.0),\n                ch.get(\"semantic_header\"),\n                keywords,\n                persons,\n                locations,\n                orthography,\n                ch.get(\"chunk_year_start\"),\n                ch.get(\"chunk_year_end\"),\n                scan_page_ids,\n                scan_tg_ids,\n                ocr_ids,\n                cast_media_ids or None,\n                internal_ids or None,\n                text,\n                emb,\n            )\n        )\n\n\n    if chunk_rows:\n        db_log(f\"Inserting {len(chunk_rows)} chunks...\")\n        execute_values(\n            cur,\n            \"\"\"insert into public.chunks (\n                book_id, file_id, book_page, tg_msg_id, position,\n                chunk_type, lang, relevance_score, topic, keywords,\n                persons, locations, orthography,\n                chunk_year_start, chunk_year_end,\n                scan_page_ids, scan_tg_msg_ids, ocr_tg_msg_ids,\n                media_ids, media_internal_ids,\n                content, embedding\n            ) values %s\"\"\",\n            chunk_rows,\n        )\n\n    conn.commit()\n    cur.close()\n    conn.close()\n    db_log(\"Book data committed to DB.\")\n\nasync def process_rag_pipeline():\n    # Ensure DB schema is ready (idempotent)\n    try:\n        ensure_db_schema()\n    except Exception as e:\n        print(f\"[DB] ‚ö†Ô∏è Schema check failed: {e}\")\n    all_md_files = sorted(glob.glob(os.path.join(INPUT_DIR, \"*.md\")))\n    valid_files = [f for f in all_md_files if \"RAG_DATA\" not in f]\n    \n    if not valid_files:\n        print(\"‚ùå No source MD files found.\")\n        return\n\n    files_to_process = valid_files[:BOOKS_LIMIT]\n    print(f\"üìÇ Found {len(valid_files)}. Processing: {len(files_to_process)}.\")\n\n    # Build unique preview filename based on first source file and current time\n    run_ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n    first_base = os.path.splitext(os.path.basename(files_to_process[0]))[0]\n    safe_base = re.sub(r\"[^0-9A-Za-z–ê-–Ø–∞-—è]+\", \"_\", first_base).strip(\"_\")\n    if len(files_to_process) == 1:\n        output_preview_file = f\"RAG_DATA_PREVIEW_{safe_base}_{run_ts}.md\"\n    else:\n        output_preview_file = f\"RAG_DATA_PREVIEW_BATCH_{run_ts}.md\"\n\n    \n    total_in, total_out = 0, 0\n    \n    client = TelegramClient(MemorySession(), API_ID, API_HASH)\n    await client.start(bot_token=BOT_TOKEN)\n    dest_ent = await client.get_entity(DEST_ID)\n    \n    preview_md = f\"# RAG DATA PREVIEW (V60.1 Fixed)\\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\"\n    \n    for md_path in files_to_process:\n        filename = os.path.basename(md_path)\n        print(f\"\\nüìò Processing: {filename}\")\n        \n        meta, full_text, page_media, page_ocr, page_scans, page_ocr_refs = parse_processed_md(md_path)\n        if not full_text: continue\n\n        book_data = {\n            \"metadata\": meta,\n            \"chunks\": [],\n            \"bibliography\": [],\n            \"footnotes\": [],\n            \"page_media\": page_media or {},\n            \"page_ocr\": page_ocr or {},\n            \"page_scans\": page_scans or {},\n            \"page_ocr_refs\": page_ocr_refs or {}\n        }\n        # USE SAFE OVERLAP FUNCTION\n        windows = create_windows_with_overlap(full_text, TEXT_WINDOW_SIZE, WINDOW_OVERLAP)\n        print(f\"   ‚úÇÔ∏è Windows: {len(windows)} (Overlap: {WINDOW_OVERLAP})\")\n        \n        context_memory = {\n            \"file_id\": 1,        \n            \"book_page\": \"1\",    \n            \"tg_msg_id\": 0,\n            \"last_chunk_text\": None\n        }\n        \n        for i, window in enumerate(windows):\n            await limiter.wait()\n            \n            model = genai.GenerativeModel(\n                model_name=MODEL_NAME,\n                generation_config={\"response_mime_type\": \"application/json\", \"temperature\": 0.0}\n            )\n            \n            try:\n                print(f\"      üì° Window {i+1}/{len(windows)}...\")\n                prompt = get_chunking_prompt(meta, window, context_memory)\n                \n                response = await asyncio.to_thread(model.generate_content, prompt)\n                \n                if response.usage_metadata:\n                    t_in = response.usage_metadata.prompt_token_count\n                    t_out = response.usage_metadata.candidates_token_count\n                    total_in += t_in; total_out += t_out\n                    print(f\"      ‚úÖ OK (Tok: {t_in}/{t_out})\")\n                \n                new_items = json.loads(response.text)\n                \n                valid_content_chunks = []\n                \n                for item in new_items:\n                    # 1. Memory Patching\n                    if item.get('file_id') is not None: context_memory['file_id'] = item['file_id']\n                    if item.get('book_page') is not None: context_memory['book_page'] = item['book_page']\n                    if item.get('tg_msg_id') is not None: context_memory['tg_msg_id'] = item['tg_msg_id']\n                    \n                    if item.get('file_id') is None: item['file_id'] = context_memory['file_id']\n                    if item.get('book_page') is None: item['book_page'] = context_memory['book_page']\n                    if item.get('tg_msg_id') is None: item['tg_msg_id'] = context_memory['tg_msg_id']\n\n                    # 2. Sort\n                    ctype = item.get('chunk_type', 'content')\n                    if ctype == 'bibliography':\n                        book_data[\"bibliography\"].append(item)\n                    elif ctype == 'footnotes_definitions':\n                        book_data[\"footnotes\"].append(item)\n                    else:\n                        book_data[\"chunks\"].append(item)\n                        if item.get('relevance_score', 0) > 0.5:\n                            valid_content_chunks.append(item)\n                \n                # 3. Update Last Chunk Context\n                if valid_content_chunks:\n                    context_memory['last_chunk_text'] = valid_content_chunks[-1]['chunk_content']\n                    snippet = context_memory['last_chunk_text'][:50].replace('\\n', ' ')\n                    print(f\"      üß† Context updated: '{snippet}...'\")\n                \n            except Exception as e:\n                print(f\"      ‚ùå Error: {e}\")\n\n\n        \n        # --- ATTACH PAGE MEDIA (IMAGES) & OCR JSON TO CHUNKS ---\n        page_media_map = book_data.get(\"page_media\") or {}\n        page_ocr_map = book_data.get(\"page_ocr\") or {}\n        page_scans_map = book_data.get(\"page_scans\") or {}\n        page_ocr_refs_map = book_data.get(\"page_ocr_refs\") or {}\n\n        def extract_tg_msg_id(link: str):\n            \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ–≤–æ–π msg_id –∏–∑ —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞ 'tg://msg_id?id=1007'.\"\"\"\n            if not link:\n                return None\n            m = re.search(r\"id=(\\d+)\", link)\n            if not m:\n                return None\n            try:\n                return int(m.group(1))\n            except ValueError:\n                return None\n\n        for section_name in (\"chunks\", \"bibliography\", \"footnotes\"):\n            for item in book_data.get(section_name, []):\n                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ —á–∞–Ω–∫—É\n                page_keys = set()\n\n                # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞\n                fid_primary = item.get(\"file_id\")\n                if fid_primary is not None:\n                    try:\n                        page_keys.add(int(fid_primary))\n                    except (TypeError, ValueError):\n                        pass\n\n                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å –µ—ë –≤–µ—Ä–Ω—É–ª–∞)\n                fid_second = item.get(\"file_id_2\")\n                if fid_second is not None:\n                    try:\n                        page_keys.add(int(fid_second))\n                    except (TypeError, ValueError):\n                        pass\n\n                # Fallback: —Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–ø–æ–ª–Ω–∏–ª–∞ file_id\n                if not page_keys:\n                    fid = item.get(\"file_id\") or item.get(\"book_page\")\n                    try:\n                        page_keys.add(int(fid))\n                    except (TypeError, ValueError):\n                        pass\n\n                if not page_keys:\n                    continue\n\n                # 1) –°–≤—è–∑—å —á–∞–Ω–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ —Å–∫–∞–Ω–∞–º–∏\n                scan_page_ids = sorted(page_keys)[:2]\n                scan_tg_ids = []\n                for pid in scan_page_ids:\n                    tg_id = None\n                    # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ, —É–∂–µ –ª–µ–∂–∞—â–∏–µ –≤ —á–∞–Ω–∫–µ\n                    try:\n                        if pid == int(item.get(\"file_id\")) and item.get(\"tg_msg_id\") not in (None, \"N/A\"):\n                            tg_id = item.get(\"tg_msg_id\")\n                        elif pid == int(item.get(\"file_id_2\")) and item.get(\"tg_msg_id_2\") not in (None, \"N/A\"):\n                            tg_id = item.get(\"tg_msg_id_2\")\n                    except Exception:\n                        pass\n\n                    if tg_id in (None, \"N/A\"):\n                        tg_id = page_scans_map.get(pid)\n\n                    if tg_id not in (None, \"N/A\"):\n                        try:\n                            tg_int = int(tg_id)\n                            if tg_int not in scan_tg_ids:\n                                scan_tg_ids.append(tg_int)\n                        except ValueError:\n                            continue\n\n                if scan_page_ids:\n                    item[\"scan_page_ids\"] = scan_page_ids\n                if scan_tg_ids:\n                    item[\"scan_tg_msg_ids\"] = scan_tg_ids\n\n                # 2) –ú–µ–¥–∏–∞–∫–æ–Ω—Ç–µ–Ω—Ç (–∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–∏ –∏ –¥—Ä. –∏–∑ APPX: Resources)\n                internal_ids = []\n                media_msg_ids = []\n                for fid_int in sorted(page_keys):\n                    page_media_list = page_media_map.get(fid_int) or []\n                    if not page_media_list:\n                        continue\n                    for mobj in page_media_list:\n                        mid = mobj.get(\"id\")\n                        if mid:\n                            internal_ids.append(mid)\n                        msg_id = extract_tg_msg_id(mobj.get(\"link\", \"\"))\n                        if msg_id is not None:\n                            media_msg_ids.append(msg_id)\n\n                if media_msg_ids:\n                    item[\"page_media_ids\"] = media_msg_ids\n                    if not item.get(\"media_ids\"):\n                        item[\"media_ids\"] = media_msg_ids\n\n                existing_internal = item.get(\"media_internal_ids\") or []\n                if not isinstance(existing_internal, (list, tuple)):\n                    existing_internal = [existing_internal]\n                if internal_ids:\n                    for mid in internal_ids:\n                        if mid not in existing_internal:\n                            existing_internal.append(mid)\n                item[\"media_internal_ids\"] = existing_internal\n\n                # 3) OCR JSON –¥–ª—è —ç—Ç–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü\n                ocr_ids = []\n                for pid in scan_page_ids:\n                    # 3a) –ñ—ë—Å—Ç–∫–∞—è –ø—Ä–∏–≤—è–∑–∫–∞ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É (OCRjson –≤ [File: ...])\n                    header_ids = page_ocr_refs_map.get(pid) or []\n                    for val in header_ids:\n                        try:\n                            vid = int(val)\n                        except (TypeError, ValueError):\n                            continue\n                        if vid not in ocr_ids:\n                            ocr_ids.append(vid)\n                    # 3b) –§–æ–ª–±—ç–∫ –ø–æ APPX: Resources\n                    page_ocr_list = page_ocr_map.get(pid) or []\n                    for mobj in page_ocr_list:\n                        msg_id = extract_tg_msg_id(mobj.get(\"link\", \"\"))\n                        if msg_id is not None and msg_id not in ocr_ids:\n                            ocr_ids.append(msg_id)\n                if ocr_ids:\n                    item[\"ocr_tg_msg_ids\"] = ocr_ids\n# === SAVE BOOK TO SUPABASE ===\n        try:\n            print(f\"üíæ Saving book '{filename}' to Supabase...\")\n            insert_book_and_related(book_data)\n        except Exception as e:\n            print(f\"[DB] ‚ùå DB Insert Error: {e}\")\n\n\n        # --- PREVIEW ---\n        preview_md += f\"# BOOK: {filename}\\n\"\n        preview_md += \"## 1. Metadata\\n\"\n        for k, v in book_data[\"metadata\"].items():\n             preview_md += f\"- **{k}**: {v}\\n\"\n        \n        preview_md += f\"\\n## 2. Content Chunks ({len(book_data['chunks'])})\\n\"\n        for idx, ch in enumerate(book_data[\"chunks\"]):\n            score = ch.get('relevance_score', 0)\n            icon = \"üü¢\" if score > 0.7 else \"üü°\" if score > 0.4 else \"üî¥\"\n            if score < RELEVANCE_THRESHOLD: icon = \"üóëÔ∏è\"\n            \n            flag = get_flag_emoji(ch.get('lang', ''))\n\n            # Location string now also shows scan + OCR JSON bindings for debug\n            scan_pages = ch.get('scan_page_ids') or []\n            scan_tg = ch.get('scan_tg_msg_ids') or []\n            ocr_ids = ch.get('ocr_tg_msg_ids') or []\n            extra_parts = []\n            if scan_pages:\n                extra_parts.append(f\"ScanPages: {scan_pages}\")\n            if scan_tg:\n                extra_parts.append(f\"ScanTG: {scan_tg}\")\n            if ocr_ids:\n                extra_parts.append(f\"OCRjson: {ocr_ids}\")\n            extra = \"\"\n            if extra_parts:\n                extra = \" | \" + \" | \".join(extra_parts)\n\n            loc_str = f\"File: {ch.get('file_id')} | Book: {ch.get('book_page')} | Ref: {ch.get('tg_msg_id')}{extra}\"\n            \n            size_icon = \"\"\n            if len(ch.get('chunk_content', '')) > MAX_CHARS_PER_CHUNK: size_icon = \"‚ö†Ô∏èLONG\"\n            \n            preview_md += f\"### [{idx+1}] {icon} {flag} {loc_str} {size_icon}\\n\"\n            preview_md += f\"- **Topic**: {ch.get('semantic_header')}\\n\"\n            preview_md += f\"- **Tags**: {', '.join(ch.get('keywords', []))}\\n\"\n            preview_md += f\"- **Persons**: {', '.join(ch.get('persons', []))}\\n\"\n            preview_md += f\"- **Locations**: {', '.join(ch.get('locations', []))}\\n\"\n            preview_md += f\"- **Orthography**: {ch.get('orthography', 'modern')}\\n\"\n            preview_md += f\"- **Score**: {score} | **Len**: {len(ch.get('chunk_content', ''))}\\n\"\n\n            media_ids = ch.get(\"media_ids\") or []\n            page_media_ids = ch.get(\"page_media_ids\") or []\n\n            def _as_str_list(values):\n                out = []\n                for v in values:\n                    if v is None:\n                        continue\n                    out.append(str(v))\n                return out\n\n            if media_ids or page_media_ids:\n                primary_str = ', '.join(_as_str_list(media_ids)) if media_ids else '‚Äî'\n                page_str = ', '.join(_as_str_list(page_media_ids)) if page_media_ids else '‚Äî'\n                preview_md += f\"- **Media (primary)**: {primary_str}\\n\"\n                preview_md += f\"- **Media (page)**: {page_str}\\n\"\n            preview_md += f\"```text\\n{ch.get('chunk_content')}\\n```\\n\\n\"\n\n        if book_data[\"bibliography\"]:\n            preview_md += f\"\\n## 3. Bibliography ({len(book_data['bibliography'])})\\n\"\n            for bib in book_data[\"bibliography\"]:\n                loc_str = f\"[Ref: {bib.get('tg_msg_id')} | P: {bib.get('book_page')}]\"\n                preview_md += f\"- {loc_str} {bib.get('chunk_content')[:300]}...\\n\"\n\n        if book_data[\"footnotes\"]:\n            preview_md += f\"\\n## 4. Footnotes ({len(book_data['footnotes'])})\\n\"\n            for fn in book_data[\"footnotes\"]:\n                 loc_str = f\"[Ref: {fn.get('tg_msg_id')} | P: {fn.get('book_page')}]\"\n                 preview_md += f\"- {loc_str} {fn.get('chunk_content')}\\n\"\n        preview_md += \"---\\n\\n\"\n\n    with open(output_preview_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(preview_md)\n    \n    summary = (\n        f\"üìä **RAG Processing V60.1**\\n\"\n        f\"Sources: {len(files_to_process)}\\n\"\n        f\"Chunks: {len(book_data['chunks'])}\\n\"\n        f\"Tokens: {total_in} in / {total_out} out\"\n    )\n    print(f\"\\n{summary}\")\n    \n    try:\n        await client.send_message(dest_ent, summary)\n        await client.send_file(dest_ent, output_preview_file, caption=\"üß† **RAG Data V60.1 (Fixed)**\")\n    except Exception as e:\n        print(f\"TG Upload Error: {e}\")\n        \n    await client.disconnect()\n\n# === RUN ===\nawait process_rag_pipeline()","metadata":{"execution":{"iopub.status.busy":"2025-12-03T20:25:32.387291Z","iopub.execute_input":"2025-12-03T20:25:32.387649Z","iopub.status.idle":"2025-12-03T20:28:13.597551Z","shell.execute_reply.started":"2025-12-03T20:25:32.387614Z","shell.execute_reply":"2025-12-03T20:28:13.595696Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[DB] 2025-12-03 20:25:32 Connecting to database for schema check...\n[DB] 2025-12-03 20:25:33 Checking extensions (vector, pg_trgm, pgcrypto)...\n[DB] 2025-12-03 20:25:33 Checking table books...\n[DB] 2025-12-03 20:25:33 Creating table books...\n[DB] 2025-12-03 20:25:34 Table books created.\n[DB] 2025-12-03 20:25:34 Checking table media...\n[DB] 2025-12-03 20:25:34 Creating table media...\n[DB] 2025-12-03 20:25:34 Table media created.\n[DB] 2025-12-03 20:25:34 Checking table chunks...\n[DB] 2025-12-03 20:25:34 Creating table chunks...\n[DB] 2025-12-03 20:25:34 Table chunks created.\n[DB] 2025-12-03 20:25:34 Checking/Updating match_chunks function...\n[DB] 2025-12-03 20:25:34 Schema check complete.\nüìÇ Found 1. Processing: 1.\n\nüìò Processing: –ü–æ–ø–æ–≤_–ú_,_–ü–æ–ø–∞–¥–∏–Ω_–ê_–û—Ç_–ö–∞–Ω—Ç–∞_–¥–æ_–¢–≤–µ—Ä–¥–æ–≥–æ_—Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è_–¥–∞–ª–µ–π,_–∫_–ª–∞–Ω–¥—à–∞—Ñ—Ç—É.md\n   ‚úÇÔ∏è Windows: 3 (Overlap: 1000)\n      üì° Window 1/3...\n      ‚úÖ OK (Tok: 4459/7613)\n      üß† Context updated: '–ü–æ–ª–æ–∂–µ–Ω–∏–µ –æ—Å–∏ –º–æ—Å—Ç–∞ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, –æ...'\n      üì° Window 2/3...\n      ‚úÖ OK (Tok: 4635/6153)\n      üß† Context updated: '–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—è –ø—Ä–æ–µ–∫—Ç 1, –∂—é—Ä–∏ —É—á–ª–æ —Ç–æ, –∫–∞–∫ –∞–≤—Ç–æ—Ä —ç—Å—Ç...'\n      üì° Window 3/3...\n      ‚úÖ OK (Tok: 3005/1767)\n      üß† Context updated: '–û–±–∞ –±—Ä–∞—Ç–∞ –æ–∫–æ–Ω—á–∏–ª–∏ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–º–µ—Å–ª–µ–Ω–Ω–æ–µ —É—á–∏–ª–∏—â...'\nüíæ Saving book '–ü–æ–ø–æ–≤_–ú_,_–ü–æ–ø–∞–¥–∏–Ω_–ê_–û—Ç_–ö–∞–Ω—Ç–∞_–¥–æ_–¢–≤–µ—Ä–¥–æ–≥–æ_—Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è_–¥–∞–ª–µ–π,_–∫_–ª–∞–Ω–¥—à–∞—Ñ—Ç—É.md' to Supabase...\n[DB] 2025-12-03 20:28:06 Connecting to database for schema check...\n[DB] 2025-12-03 20:28:06 Checking extensions (vector, pg_trgm, pgcrypto)...\n[DB] 2025-12-03 20:28:07 Checking table books...\n[DB] 2025-12-03 20:28:07 Table books already exists.\n[DB] 2025-12-03 20:28:07 Checking table media...\n[DB] 2025-12-03 20:28:07 Table media already exists.\n[DB] 2025-12-03 20:28:07 Checking table chunks...\n[DB] 2025-12-03 20:28:07 Table chunks already exists.\n[DB] 2025-12-03 20:28:07 Checking/Updating match_chunks function...\n[DB] 2025-12-03 20:28:07 Schema check complete.\n[DB] 2025-12-03 20:28:08 Inserting new book row...\n[DB] 2025-12-03 20:28:08 Book inserted with id=5f1c172d-ead3-4579-a29b-a437f9e230d6\n[DB] 2025-12-03 20:28:08 Inserting 11 media rows...\n[DB] 2025-12-03 20:28:12 Inserting 20 chunks...\n[DB] 2025-12-03 20:28:13 Book data committed to DB.\n\nüìä **RAG Processing V60.1**\nSources: 1\nChunks: 20\nTokens: 12099 in / 15533 out\n","output_type":"stream"}],"execution_count":4}]}