{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# –Ø—á–µ–π–∫–∞ 1: –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û–ï –û–ë–ù–û–í–õ–ï–ù–ò–ï –ë–ò–ë–õ–ò–û–¢–ï–ö\n!apt-get update -qq\n!apt-get install -y -qq djvulibre-bin\n# –§–ª–∞–≥ -U –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ gemini-1.5-flash\n!pip install -U -q google-generativeai telethon pymupdf tqdm pillow\n!echo \"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –û–ë–ù–û–í–õ–ï–ù–´. –¢–µ–ø–µ—Ä—å –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤–∏–¥–Ω–∞.\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-27T21:53:54.825315Z","iopub.execute_input":"2025-11-27T21:53:54.825574Z","iopub.status.idle":"2025-11-27T21:54:31.396873Z","shell.execute_reply.started":"2025-11-27T21:53:54.825553Z","shell.execute_reply":"2025-11-27T21:54:31.395539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# –Ø—á–µ–π–∫–∞ –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\n\nprint(\"üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...\")\ntry:\n    user_secrets = UserSecretsClient()\n    GENAI_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n    genai.configure(api_key=GENAI_KEY)\n    \n    available_models = []\n    for m in genai.list_models():\n        if 'generateContent' in m.supported_generation_methods:\n            print(f\"   ‚úÖ –î–æ—Å—Ç—É–ø–Ω–∞: {m.name}\")\n            available_models.append(m.name)\n            \n    if not available_models:\n        print(\"‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API Key.\")\n    else:\n        print(f\"\\n–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(available_models)}\")\n\nexcept Exception as e:\n    print(f\"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}\")\n    print(\"üí° –°–û–í–ï–¢: –ù–∞–∂–º–∏—Ç–µ –≤ –º–µ–Ω—é 'Run' -> 'Restart Session', —á—Ç–æ–±—ã –æ–±–Ω–æ–≤–∏—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫–∏.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === –Ø–ß–ï–ô–ö–ê 2: V57.1 (FULL META + JSON REPAIR) ===\nimport os\nimport time\nimport json\nimport asyncio\nimport shutil\nimport re\nimport fitz  # PyMuPDF\nimport subprocess\nimport google.generativeai as genai\nfrom PIL import Image\nfrom datetime import datetime\nfrom telethon import TelegramClient, events, functions, types\nfrom telethon.sessions import MemorySession\nfrom kaggle_secrets import UserSecretsClient\nfrom tqdm.notebook import tqdm\nfrom google.api_core.exceptions import ResourceExhausted, ServiceUnavailable, InternalServerError\n\n# === SAFETY ===\nImage.MAX_IMAGE_PIXELS = None \n\n# === SETTINGS ===\nSOURCE_USERNAME = \"sendDoc39\"\n# –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ø—Ä–æ–±—É–µ–º 5, –ø—Ä–∏ –æ—à–∏–±–∫–µ 3, –ø—Ä–∏ —Å–æ–≤—Å–µ–º –æ—à–∏–±–∫–µ 1\nBATCH_STRATEGY = [5, 3, 1]  \n\nGLOBAL_REQUEST_DELAY = 30.0  \n\nZOOM_FACTOR = 2.0           \nMARKER_EMOJI = \"‚úÖ\"\nMAX_EMPTY_GAP = 5\nSAFETY_LIMIT = 500\n\nDJVU_TARGET_SIZE = 2560     \nCROP_PADDING = 15           \nSNAP_THRESHOLD = 30         \n\n# --- PRODUCTION SETTINGS ---\nDEBUG_PAGE_LIMIT = 5      \n# ---------------------------\n\nuser_secrets = UserSecretsClient()\nTEMP_DIR = \"./temp_processing\"\nCROPS_DIR = \"./temp_crops\"\n\nLOG_BUFFER = []\n\ndef log(text):\n    tqdm.write(text)\n    LOG_BUFFER.append(text)\n\nasync def send_report(client, dest_entity):\n    report_text = \"\\n\".join(LOG_BUFFER)\n    if len(report_text) > 4000: report_text = report_text[-3900:]\n    try:\n        if dest_entity: await client.send_message(dest_entity, f\"üìù **Report:**\\n\\n...{report_text}\")\n    except: pass\n\n# === JSON REPAIR KIT ===\ndef repair_json_content(text):\n    \"\"\"\n    –õ–µ—á–∏—Ç –æ—à–∏–±–∫—É 'Invalid \\escape', —ç–∫—Ä–∞–Ω–∏—Ä—É—è –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–ª–µ—à–∏,\n    –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—è –≤–∞–ª–∏–¥–Ω—ã–µ JSON-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (\\n, \\t, \\\", \\\\).\n    \"\"\"\n    # –ü–∞—Ç—Ç–µ—Ä–Ω –Ω–∞—Ö–æ–¥–∏—Ç backslash, –∑–∞ –∫–æ—Ç–æ—Ä—ã–º –ù–ï —Å–ª–µ–¥—É–µ—Ç –≤–∞–ª–∏–¥–Ω—ã–π escape-—Å–∏–º–≤–æ–ª\n    # Valid JSON escapes: \", \\, /, b, f, n, r, t, u\n    pattern = r'\\\\(?![/u\"bfnrt\\\\])'\n    return re.sub(pattern, r'\\\\\\\\', text)\n\n# === MODEL MANAGER ===\nclass ModelRotator:\n    def __init__(self, api_keys):\n        if isinstance(api_keys, str): self.api_keys = [api_keys]\n        else: self.api_keys = api_keys\n        self.current_key_idx = 0\n        self._configure_current_key()\n        self.last_request_ts = 0\n        \n        # Priority Models\n        self.models_list = [\n            \"models/gemini-2.5-flash\",      # Priority 1\n            \"models/gemini-2.0-flash-exp\",  # Priority 2\n            \"models/gemini-1.5-flash\",      # Priority 3\n        ]\n        log(f\"üß† Models initialized. Main: {self.models_list[0]}. Delay: {GLOBAL_REQUEST_DELAY}s.\")\n\n    def _configure_current_key(self):\n        genai.configure(api_key=self.api_keys[self.current_key_idx])\n\n    async def _enforce_global_pace(self):\n        now = time.time()\n        elapsed = now - self.last_request_ts\n        if elapsed < GLOBAL_REQUEST_DELAY:\n            wait_time = GLOBAL_REQUEST_DELAY - elapsed + 1\n            await asyncio.sleep(wait_time)\n        self.last_request_ts = time.time()\n\n    async def generate_content(self, content):\n        for i, model_name in enumerate(self.models_list):\n            await self._enforce_global_pace()\n            \n            # Low temp for structural adherence\n            model = genai.GenerativeModel(\n                model_name=model_name,\n                safety_settings=[{\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"}],\n                generation_config={\"response_mime_type\": \"application/json\", \"temperature\": 0.0}\n            )\n            \n            try:\n                log(f\"      üì° [{model_name}] Sending...\")\n                t_start = time.time()\n                response = await asyncio.to_thread(model.generate_content, content)\n                dur = time.time() - t_start\n                \n                usage = {\"in\": 0, \"out\": 0}\n                if response.usage_metadata:\n                    usage[\"in\"] = response.usage_metadata.prompt_token_count\n                    usage[\"out\"] = response.usage_metadata.candidates_token_count\n                \n                return response.text, usage, model_name, dur\n            \n            except (ResourceExhausted, ServiceUnavailable, InternalServerError) as e:\n                log(f\"      ‚ö†Ô∏è Limit/Error on {model_name}.\")\n                if i == len(self.models_list) - 1:\n                    log(\"      ‚ùå ALL MODELS EXHAUSTED. Aborting execution.\")\n                    raise e \n                log(\"      ‚û°Ô∏è Switching to backup model...\")\n                continue\n                \n            except Exception as e:\n                if \"429\" in str(e):\n                    log(f\"      ‚ö†Ô∏è 429 Too Many Requests on {model_name}.\")\n                    if i == len(self.models_list) - 1:\n                        log(\"      ‚ùå ALL MODELS EXHAUSTED (429). Aborting.\")\n                        raise e\n                    continue\n                log(f\"      ‚ùå Unexpected Error ({model_name}): {e}\")\n                raise e\n\n        raise Exception(\"Unexpected logic flow in ModelRotator\")\n\n# === INIT ===\nmodel_rotator = None\ntry:\n    GENAI_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n    model_rotator = ModelRotator(GENAI_KEY)\nexcept Exception as e: log(f\"‚ùå Init Error: {e}\")\n\n# === AI LOGIC (UPDATED PROMPT V57.1) ===\ndef get_gemini_prompt(is_first_batches, previous_context=None):\n    base_prompt = \"\"\"\n    TASK: OCR & Layout Analysis (Spread vs Journal Page) with Rich Formatting.\n    \"\"\"\n    \n    if previous_context:\n        base_prompt += f\"\"\"\n    CONTEXT FROM PREV BATCH: \"...{previous_context}...\"\n    \"\"\"\n\n    base_prompt += \"\"\"\n    OUTPUT FORMAT (JSON):\n    {\n      \"pages\": [\n        {\n           \"book_page_num\": \"string\",         // PRINTED page number.\n           \"classification_code\": \"string\",   // UDC/BBC codes (e.g. \"–£–î–ö...\").\n           \"main_text\": \"string\",             // Markdown text with headers/bold/italic.\n           \"footnotes\": \"string\",             // Isolated footnotes.\n           \"marginalia\": \"string\",            // Headers/Running titles.\n           \"media_objects\": [\n              { \"id\": \"string\", \"coordinates\": [y,x,y,x], \"type\": \"string\", \"caption_ru\": \"string\" }\n           ]\n        }\n      ],\n    \"\"\"\n    \n    if is_first_batches:\n        base_prompt += \"\"\"\n      \"bibliographic_data\": {\n          \"title\": \"string\",\n          \"authors\": [\"string\"],\n          \"co_authors\": [\"string\"],\n          \"publisher\": \"string\",\n          \"year\": \"string\",\n          \"isbn\": \"string\",\n          \"document_type\": \"string (book, article, official_document, archive_record, other)\",\n          \"historical_period_description\": \"string (–Ω–∞ —Ä—É—Å—Å–∫–æ–º)\",\n          \"historical_year_start\": \"int or null\",\n          \"historical_year_end\": \"int or null\",\n          \"summary\": \"string (–Ω–∞ —Ä—É—Å—Å–∫–æ–º)\"\n      }\n    }\n    \"\"\"\n    else:\n        base_prompt += '  \"batch_metadata\": { \"status\": \"cont\" }\\n}'\n\n    # --- RULES V57.1 ---\n    base_prompt += \"\"\"\n    CRITICAL RULES:\n    \n    1. **LAYOUT & ZONES**:\n       - Check for **Single Page** vs **Spread** (2 pages).\n       - If **Single Page** (Journal Style): Detect horizontal zones (Top Article end / Bottom Article start). Use `***` separator.\n       - Do not merge columns across different articles.\n\n    2. **RICH MARKDOWN FORMATTING (HIERARCHY)**:\n       - **Headers**: Detect font size/boldness. Use `##` for Main Titles, `###` for Subtitles.\n       - **Emphasis**: Use `**bold**` for bold text. Use `*italic*` for italics.\n       - **Structure**: Preserve paragraph breaks.\n\n    3. **METADATA**:\n       - Extract full bibliographic data if available (first batch).\n       - Use **Russian language** for descriptions and summaries.\n       - If a value is unknown, use `null` or empty string.\n\n    4. **FOOTNOTES**:\n       - CUT footnote text from main body and PASTE into `footnotes`.\n       \n    5. **JSON SAFETY**:\n       - Do not use raw backslashes inside strings unless escaping. Avoid LaTeX macros if possible, use Unicode.\n    \"\"\"\n    return base_prompt\n\nasync def process_single_batch(image_paths, page_nums, prev_ctx_text):\n    if not model_rotator: raise Exception(\"No models\")\n    \n    is_first_batch = (page_nums[0] == 1)\n    \n    prompt = get_gemini_prompt(is_first_batch, prev_ctx_text)\n    content = [prompt]\n    loaded_imgs = []\n    \n    for p in image_paths:\n        img = Image.open(p)\n        w, h = img.size\n        if w > 3072 or h > 3072:\n            img.thumbnail((3072, 3072), Image.Resampling.LANCZOS)\n        loaded_imgs.append(img)\n        content.append(img)\n        \n    text_resp, usage, model_name, duration = await model_rotator.generate_content(content)\n    \n    clean_text = text_resp.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n    if clean_text.startswith(\"json\"): clean_text = clean_text[4:].strip()\n    match = re.search(r'\\{.*\\}', clean_text, re.DOTALL)\n    if match: clean_text = match.group(0)\n\n    try:\n        # 1. –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ \"–∫–∞–∫ –µ—Å—Ç—å\"\n        data = json.loads(clean_text) \n    except Exception as e1:\n        # 2. –ü–æ–ø—ã—Ç–∫–∞ –ª–µ—á–µ–Ω–∏—è JSON (backslashes)\n        log(f\"      ‚ö†Ô∏è JSON Error 1: {e1}. Repairing...\")\n        repaired_text = repair_json_content(clean_text)\n        try:\n            data = json.loads(repaired_text)\n            log(\"      üõ†Ô∏è Repair Successful!\")\n        except Exception as e2:\n            log(f\"      ‚ùå Repair Failed: {e2}. Raw text len: {len(clean_text)}\")\n            raise Exception(\"JSON Parsing Failed completely\") \n    \n    batch_meta = data.get(\"bibliographic_data\", data.get(\"batch_metadata\", None))\n    return data.get(\"pages\", []), loaded_imgs, usage, model_name, duration, batch_meta\n\ndef crop_images(pil_imgs, pages_data):\n    crops = []\n    if not os.path.exists(CROPS_DIR): os.makedirs(CROPS_DIR, exist_ok=True)\n    \n    for i, page_data in enumerate(pages_data):\n        if i >= len(pil_imgs): src = pil_imgs[-1]\n        else: src = pil_imgs[i]\n        \n        media_list = page_data.get(\"media_objects\", [])\n        w, h = src.size\n        \n        # Cover Guard\n        is_cover_page = False\n        cover_object = None\n        for m in media_list:\n            if m.get(\"type\") == \"cover\" or m.get(\"id\") == \"book_cover\":\n                is_cover_page = True\n                cover_object = m\n                break\n        \n        if is_cover_page and cover_object:\n            cover_object[\"coordinates\"] = [0, 0, 1000, 1000]\n            cover_object[\"id\"] = \"book_cover\"\n            media_list = [cover_object]\n        \n        for meta in media_list:\n            try:\n                ymin, xmin, ymax, xmax = meta[\"coordinates\"]\n                left = (xmin/1000)*w - CROP_PADDING\n                top = (ymin/1000)*h - CROP_PADDING\n                right = (xmax/1000)*w + CROP_PADDING\n                bottom = (ymax/1000)*h + CROP_PADDING\n                \n                left = max(0, left); top = max(0, top)\n                right = min(w, right); bottom = min(h, bottom)\n                \n                if (right-left < 30) or (bottom-top < 30): continue\n                \n                crop = src.crop((left, top, right, bottom))\n                \n                raw_id = meta.get(\"id\", f\"media_{int(time.time())}_{i}\")\n                if meta.get(\"type\") == \"cover\": media_id = \"book_cover\"\n                else: media_id = re.sub(r'[^a-zA-Z0-9_]', '', raw_id)\n                \n                c_name = f\"{media_id}.png\"\n                c_path = os.path.join(CROPS_DIR, c_name)\n                crop.save(c_path)\n                \n                caption = f\"{meta.get('type', 'img').upper()}: {meta.get('caption_ru', '')}\"\n                crops.append((c_path, caption, media_id))\n            except: pass\n    return crops\n\n# === TELEGRAM & UTILS ===\nasync def get_last_id(client, entity):\n    try:\n        full = await client(functions.channels.GetFullChannelRequest(entity))\n        match = re.search(r\"\\[LastID:\\s*(\\d+)\\]\", full.full_chat.about or \"\")\n        if match: return int(match.group(1))\n    except: pass\n    return None\n\nasync def update_last_id(client, entity, new_id):\n    try:\n        full = await client(functions.channels.GetFullChannelRequest(entity))\n        about = full.full_chat.about or \"\"\n        tag = f\"[LastID: {new_id}]\"\n        if \"[LastID:\" in about: new_about = re.sub(r\"\\[LastID:\\s*\\d+\\]\", tag, about)\n        else: new_about = f\"{about}\\n\\n{tag}\".strip()\n        if new_about != about: await client(functions.messages.EditChatAboutRequest(peer=entity, about=new_about))\n    except: pass\n\nasync def is_already_processed(message):\n    try:\n        if not message.reactions: return False\n        for r in message.reactions.results:\n            emoji = r.reaction.emoticon if hasattr(r.reaction, 'emoticon') else str(r.reaction)\n            if emoji == MARKER_EMOJI and r.chosen: return True\n    except: pass\n    return False\n\nasync def mark_processed(client, msg):\n    try:\n        await client(functions.messages.SendReactionRequest(peer=msg.peer_id, msg_id=msg.id, reaction=[types.ReactionEmoji(emoticon=MARKER_EMOJI)]))\n    except: pass\n\ndef render_pdf(path):\n    doc = fitz.open(path)\n    mat = fitz.Matrix(ZOOM_FACTOR, ZOOM_FACTOR)\n    limit = len(doc)\n    if DEBUG_PAGE_LIMIT:\n        limit = min(len(doc), DEBUG_PAGE_LIMIT)\n    for i in range(limit):\n        page = doc.load_page(i)\n        pix = page.get_pixmap(matrix=mat, alpha=False)\n        out = os.path.join(TEMP_DIR, f\"page_{i+1:04d}.png\")\n        pix.save(out)\n        yield out, i+1, len(doc)\n    doc.close()\n\ndef render_djvu(path):\n    safe = os.path.join(TEMP_DIR, \"temp.djvu\")\n    shutil.move(path, safe)\n    try:\n        subprocess.run([\n            'ddjvu', '-format=tiff', f'-size={DJVU_TARGET_SIZE}x{DJVU_TARGET_SIZE}', \n            '-eachpage', safe, os.path.join(TEMP_DIR, \"p_%04d.tif\")\n        ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n    except Exception as e:\n        log(f\"‚ùå DjVu Render Error: {e}\")\n        return\n\n    files = sorted([f for f in os.listdir(TEMP_DIR) if f.endswith(\".tif\")])\n    if DEBUG_PAGE_LIMIT: files = files[:DEBUG_PAGE_LIMIT]\n    for i, f in enumerate(tqdm(files, desc=\"Conv\", unit=\"pg\", leave=False)):\n        full = os.path.join(TEMP_DIR, f)\n        try:\n            with Image.open(full) as img:\n                png = full.replace(\".tif\", \".png\")\n                img.save(png, \"PNG\")\n                yield png, i+1, len(files)\n            os.remove(full)\n        except: pass\n    if os.path.exists(safe): os.remove(safe)\n\n# === MAIN ===\nasync def main_logic():\n    log(f\"üöÄ **Launch V57.1 (Full Meta + JSON Repair):** {datetime.now().strftime('%H:%M:%S')}\")\n    \n    try:\n        API_ID = int(user_secrets.get_secret(\"TELEGRAM_API_ID\"))\n        API_HASH = user_secrets.get_secret(\"TELEGRAM_API_HASH\")\n        BOT_TOKEN = user_secrets.get_secret(\"TELEGRAM_BOT_TOKEN\")\n        DEST_ID = int(user_secrets.get_secret(\"DEST_CHANNEL_ID\"))\n    except: return log(\"‚ùå Secrets missing.\")\n\n    for d in [TEMP_DIR, CROPS_DIR]:\n        if not os.path.exists(d): os.makedirs(d, exist_ok=True)\n\n    client = TelegramClient(MemorySession(), API_ID, API_HASH)\n    await client.start(bot_token=BOT_TOKEN)\n    log(\"‚úÖ Telegram OK.\")\n    if not model_rotator: return await client.disconnect()\n\n    try:\n        src_ent = await client.get_entity(SOURCE_USERNAME)\n        dst_ent = await client.get_entity(DEST_ID)\n    except: return await client.disconnect()\n\n    start_id = await get_last_id(client, src_ent) or 1\n    curr_id = start_id + 1\n    processed = 0\n    empty_streak = 0\n    t_in, t_out = 0, 0\n    pbar = tqdm(total=SAFETY_LIMIT, desc=\"Scan\", unit=\"msg\")\n\n    while processed < SAFETY_LIMIT:\n        if empty_streak >= MAX_EMPTY_GAP: log(\"üèÅ End.\"); break\n        try:\n            msgs = await client.get_messages(src_ent, ids=[curr_id])\n            msg = msgs[0] if msgs else None\n            \n            if not msg or isinstance(msg, types.MessageEmpty):\n                empty_streak += 1; curr_id += 1; pbar.update(1); continue\n            \n            if await is_already_processed(msg):\n                await update_last_id(client, src_ent, msg.id)\n                curr_id += 1; pbar.update(1); continue\n\n            fname = getattr(msg.file, 'name', None)\n            if not fname: curr_id += 1; pbar.update(1); continue\n            ext = fname.split('.')[-1].lower()\n            if ext not in ['pdf', 'djvu']: curr_id += 1; pbar.update(1); continue\n\n            empty_streak = 0\n            log(f\"\\nüìò **{fname}** (ID: {msg.id})\")\n            fpath = os.path.join(TEMP_DIR, fname)\n            await msg.download_media(file=fpath)\n            \n            iter_pages = render_pdf(fpath) if ext == 'pdf' else render_djvu(fpath)\n            \n            # === FULL METADATA STRUCT V57 ===\n            full_bibliographic_data = {\n                \"isbn\": \"\",\n                \"title\": \"\",\n                \"authors\": [],\n                \"co_authors\": [],\n                \"publisher\": \"\",\n                \"year\": \"\",\n                \"document_type\": \"\",\n                \"historical_period_description\": \"\",\n                \"historical_year_start\": \"\",\n                \"historical_year_end\": \"\",\n                \"summary\": \"\",\n                \"source_telegram_channel_id\": str(dst_ent.id),\n                \"source_origin_msg_id\": str(msg.id)\n            }\n            \n            global_text_blocks = []      \n            global_resources = []        \n            global_footnotes = []\n            global_marginalia = set()\n            \n            last_batch_tail_text = None \n            current_batch_size = BATCH_STRATEGY[0]\n            \n            generated_pages_buffer = []\n            for p in iter_pages: generated_pages_buffer.append(p)\n            \n            cursor = 0\n            total_pages = len(generated_pages_buffer)\n            book_failed = False\n\n            while cursor < total_pages:\n                end = min(cursor + current_batch_size, total_pages)\n                current_slice = generated_pages_buffer[cursor : end]\n                slice_paths = [x[0] for x in current_slice]\n                slice_idxs = [x[1] for x in current_slice]\n                \n                log(f\"   üîÑ Batch {slice_idxs[0]}-{slice_idxs[-1]} (Size: {len(slice_paths)})...\")\n                \n                try:\n                    pages_list, imgs, usg, m_name, duration, meta = await process_single_batch(\n                        slice_paths, slice_idxs, last_batch_tail_text\n                    )\n                    \n                    log(f\"      ‚úÖ [{m_name}] OK in {duration:.1f}s\")\n                    t_in += usg.get(\"in\",0); t_out += usg.get(\"out\",0)\n                    \n                    # === BATCH UPLOAD ===\n                    page_msg_ids = {} \n                    ALBUM_CHUNK_SIZE = 10\n                    for i in range(0, len(slice_paths), ALBUM_CHUNK_SIZE):\n                        sub_paths = slice_paths[i : i + ALBUM_CHUNK_SIZE]\n                        sub_idxs = slice_idxs[i : i + ALBUM_CHUNK_SIZE]\n                        try:\n                            sent_msgs = await client.send_file(dst_ent, file=sub_paths, force_document=True)\n                            if not isinstance(sent_msgs, list): sent_msgs = [sent_msgs]\n                            for sent_m, p_idx in zip(sent_msgs, sub_idxs):\n                                page_msg_ids[p_idx] = sent_m.id\n                            await asyncio.sleep(2) \n                        except: pass\n                    # ====================\n\n                    if meta and isinstance(meta, dict):\n                        for k, v in meta.items():\n                            if k in full_bibliographic_data and v:\n                                # Overwrite empty fields, don't overwrite existing valid data if new one is partial\n                                curr = full_bibliographic_data[k]\n                                if not curr: full_bibliographic_data[k] = v\n\n                    crops = crop_images(imgs, pages_list)\n                    for cp_path, cp_cap, cp_id in crops:\n                        msg_link = \"local_error\"\n                        if os.path.exists(cp_path):\n                            try:\n                                c_msg = await client.send_file(dst_ent, cp_path, caption=f\"üñº {cp_cap}\\nüÜî {cp_id}\", force_document=False)\n                                msg_link = f\"tg://msg_id?id={c_msg.id}\"\n                            except: pass\n                            finally:\n                                if os.path.exists(cp_path): os.remove(cp_path)\n                                await asyncio.sleep(2)\n                        global_resources.append({\"id\": cp_id, \"caption\": cp_cap, \"link\": msg_link})\n\n                    current_img_idx = 0\n                    for i, page_data in enumerate(pages_list):\n                        if i < len(slice_idxs): container_p = slice_idxs[i]\n                        else: container_p = slice_idxs[-1]\n                        \n                        raw_text = page_data.get(\"main_text\", \"\")\n                        book_p = page_data.get(\"book_page_num\", None)\n                        class_code = page_data.get(\"classification_code\", None)\n                        \n                        tg_ref_id = page_msg_ids.get(container_p, \"N/A\")\n                        \n                        if page_data.get(\"footnotes\"): \n                            global_footnotes.append(f\"[{container_p}] {page_data['footnotes']}\")\n                        \n                        if page_data.get(\"marginalia\"): \n                            cleaned_marg = page_data[\"marginalia\"].strip()\n                            if len(cleaned_marg) > 3: global_marginalia.add(cleaned_marg)\n                        \n                        final_page_text = raw_text.strip()\n                        if class_code:\n                            final_page_text = f\"**{class_code}**\\n\\n\" + final_page_text\n\n                        global_text_blocks.append({\n                            \"container_idx\": container_p,\n                            \"book_num\": book_p,\n                            \"tg_ref\": tg_ref_id, \n                            \"text\": final_page_text\n                        })\n                    \n                    if global_text_blocks:\n                        last_text_content = global_text_blocks[-1][\"text\"]\n                        last_batch_tail_text = last_text_content[-1000:]\n                    \n                    for p in slice_paths:\n                        if os.path.exists(p): os.remove(p)\n                        \n                    cursor += len(slice_paths)\n                    \n                except Exception as e:\n                    log(f\"      ‚ö†Ô∏è Failed: {str(e)[:100]}\")\n                    if \"ResourceExhausted\" in str(e) or \"429\" in str(e):\n                        log(\"      ‚ùå CRITICAL API LIMIT. Stopping book.\")\n                        book_failed = True\n                        break\n                    \n                    new_size = current_batch_size\n                    for s in BATCH_STRATEGY:\n                        if s < current_batch_size:\n                            new_size = s\n                            break\n                    if new_size == current_batch_size:\n                        cursor += 1 \n                        current_batch_size = 1\n                    else:\n                        log(f\"      ‚¨áÔ∏è Reducing batch size: {current_batch_size} -> {new_size}\")\n                        current_batch_size = new_size\n\n            if book_failed:\n                await client.send_message(dst_ent, f\"‚ùå **Aborted:** {fname} (API Limits)\")\n                await update_last_id(client, src_ent, msg.id)\n                continue\n\n            # === GLOBAL ASSEMBLY ===\n            final_md = f\"# {fname}\\n\\n\"\n            \n            final_md += \"## Metadata\\n\"\n            # Explicit order of ALL keys\n            meta_order = [\n                \"isbn\", \"title\", \"authors\", \"co_authors\", \"publisher\", \"year\", \n                \"document_type\", \"historical_period_description\", \n                \"historical_year_start\", \"historical_year_end\", \"summary\",\n                \"source_telegram_channel_id\", \"source_origin_msg_id\"\n            ]\n            \n            for k in meta_order:\n                val = full_bibliographic_data.get(k, \"\")\n                if isinstance(val, list):\n                    val_str = \", \".join([str(x) for x in val if x]) # Filter empty strings\n                else:\n                    val_str = str(val) if val is not None else \"\"\n                \n                # Print key even if empty, but NO \"N/A\"\n                final_md += f\"- **{k}**: {val_str}\\n\"\n            \n            final_md += \"\\n## Content\\n\\n\"\n            \n            joined_content = \"\"\n            for i, block_data in enumerate(global_text_blocks):\n                c_idx = block_data[\"container_idx\"]\n                b_num = block_data[\"book_num\"]\n                tg_id = block_data.get(\"tg_ref\", \"N/A\")\n                text = block_data[\"text\"]\n                \n                label = f\"> `[File: {c_idx} | Ref: {tg_id}]`\"\n                if b_num: label = f\"> `[File: {c_idx} | Book: {b_num} | Ref: {tg_id}]`\"\n                \n                page_marker = f\"\\n\\n{label}\\n\"\n                if i > 0: joined_content += page_marker + text\n                else: joined_content += f\"{label}\\n\" + text\n            \n            final_md += joined_content.strip()\n\n            if global_resources:\n                final_md += \"\\n\\n## APPX: Resources\\n\"\n                for res in global_resources:\n                    final_md += f\"- **ID**: `{res['id']}`\\n\"\n                    final_md += f\"  - **Caption**: {res['caption']}\\n\"\n                    final_md += f\"  - **Link**: {res['link']}\\n\"\n            \n            if global_footnotes:\n                final_md += \"\\n\\n## APPX: Footnotes\\n\" + \"\\n\\n\".join(global_footnotes)\n            \n            if global_marginalia:\n                final_md += \"\\n\\n## APPX: Marginalia\\n\" + \"\\n\".join(sorted(list(global_marginalia)))\n\n            md_out = os.path.join(TEMP_DIR, f\"{os.path.splitext(fname)[0]}.md\")\n            with open(md_out, \"w\", encoding=\"utf-8\") as f: f.write(final_md)\n            await client.send_file(dst_ent, md_out, caption=f\"üìö **MD FULL** (Tok: {t_in+t_out})\", force_document=True)\n            \n            await mark_processed(client, msg)\n            await update_last_id(client, src_ent, msg.id)\n            log(f\"   üéâ Done.\")\n\n        except Exception as e:\n            log(f\"‚ùå Error ID {curr_id}: {e}\")\n            import traceback; traceback.print_exc()\n        \n        finally:\n            if os.path.exists(CROPS_DIR):\n                shutil.rmtree(CROPS_DIR)\n                os.makedirs(CROPS_DIR, exist_ok=True)\n            if os.path.exists(TEMP_DIR):\n                for f in os.listdir(TEMP_DIR):\n                    if not f.endswith(\".md\"):\n                        try: os.remove(os.path.join(TEMP_DIR, f))\n                        except: pass\n                        \n        curr_id += 1; processed += 1; pbar.update(1)\n\n    log(f\"\\nüèÅ End. Total Tokens: {t_in}/{t_out}\")\n    await send_report(client, dst_ent)\n    await client.disconnect()\n\n# === RUN ===\nprint(\"üü° Pre-flight check...\")\ntry:\n    _ = UserSecretsClient().get_secret(\"TELEGRAM_API_ID\")\n    print(\"üü¢ Starting main_logic...\")\n    await main_logic()\nexcept Exception as e: print(f\"üî¥ FAIL: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T21:55:09.850573Z","iopub.execute_input":"2025-11-27T21:55:09.852270Z","iopub.status.idle":"2025-11-27T21:57:10.289639Z","shell.execute_reply.started":"2025-11-27T21:55:09.852217Z","shell.execute_reply":"2025-11-27T21:57:10.288379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === –Ø–ß–ï–ô–ö–ê 3: V59 (STRICTER LIMITS, SMALLER WINDOWS, KEEP OVERLAP) ===\n\nimport os\nimport time\nimport json\nimport asyncio\nimport glob\nimport re\nimport google.generativeai as genai\nfrom datetime import datetime\nfrom kaggle_secrets import UserSecretsClient\nfrom telethon import TelegramClient\nfrom telethon.sessions import MemorySession\n\n# === SETTINGS ===\nINPUT_DIR = \"./temp_processing\"\nOUTPUT_PREVIEW_FILE = \"RAG_DATA_PREVIEW.md\"\n\n# API Safety\nGLOBAL_REQUEST_DELAY = 30.0  \n# –£–º–µ–Ω—å—à–∞–µ–º –æ–∫–Ω–æ, —á—Ç–æ–±—ã –º–æ–¥–µ–ª–∏ –±—ã–ª–æ –ø—Ä–æ—â–µ —Å–æ–±–ª—é–¥–∞—Ç—å –ª–∏–º–∏—Ç —á–∞–Ω–∫–æ–≤\nTEXT_WINDOW_SIZE = 8000     \n\n# Chunk Constraints\nMAX_CHARS_PER_CHUNK = 2000 \n\n# Logic\nRELEVANCE_THRESHOLD = 0.3 \n\n# Debug Limit\nBOOKS_LIMIT = 1 \n\n# === AUTH SETUP ===\nuser_secrets = UserSecretsClient()\ntry:\n    GENAI_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n    genai.configure(api_key=GENAI_KEY)\n    \n    API_ID = int(user_secrets.get_secret(\"TELEGRAM_API_ID\"))\n    API_HASH = user_secrets.get_secret(\"TELEGRAM_API_HASH\")\n    BOT_TOKEN = user_secrets.get_secret(\"TELEGRAM_BOT_TOKEN\")\n    DEST_ID = int(user_secrets.get_secret(\"DEST_CHANNEL_ID\"))\nexcept:\n    print(\"‚ùå Secrets missing\")\n\nMODEL_NAME = \"models/gemini-2.5-flash\"\n\n# === UTILS ===\nclass RateLimiter:\n    def __init__(self, delay):\n        self.delay = delay\n        self.last_call = 0\n    \n    async def wait(self):\n        now = time.time()\n        elapsed = now - self.last_call\n        if elapsed < self.delay:\n            wait_time = self.delay - elapsed + 1\n            print(f\"      ‚è≥ Pace: waiting {wait_time:.1f}s...\")\n            await asyncio.sleep(wait_time)\n        self.last_call = time.time()\n\nlimiter = RateLimiter(GLOBAL_REQUEST_DELAY)\n\ndef get_flag_emoji(lang_code):\n    flags = {\n        \"ru\": \"üá∑üá∫\", \"de\": \"üá©üá™\", \"en\": \"üá¨üáß\", \n        \"pl\": \"üáµüá±\", \"fr\": \"üá´üá∑\", \"it\": \"üáÆüáπ\",\n        \"lt\": \"üá±üáπ\"\n    }\n    return flags.get(lang_code, \"üåê\")\n\ndef parse_processed_md(filepath):\n    if not os.path.exists(filepath): return None, None\n    with open(filepath, 'r', encoding='utf-8') as f: content = f.read()\n    \n    meta = {}\n    meta_match = re.search(r'## Metadata\\n(.*?)\\n\\n## Content', content, re.DOTALL)\n    if meta_match:\n        lines = meta_match.group(1).split('\\n')\n        for line in lines:\n            if line.strip().startswith('- **'):\n                match_row = re.search(r'\\*\\*(\\w+)\\*\\*: (.*)', line)\n                if match_row: \n                    key = match_row.group(1)\n                    val = match_row.group(2).strip()\n                    if val.lower() != \"null\" and val != \"\":\n                        meta[key] = val\n    \n    text_match = re.search(r'## Content\\n(.*?)(\\n## APPX|$)', content, re.DOTALL)\n    clean_text = text_match.group(1).strip() if text_match else \"\"\n    return meta, clean_text\n\ndef create_smart_windows(text, max_size):\n    windows = []\n    current_window = \"\"\n    paragraphs = text.split('\\n\\n')\n    for para in paragraphs:\n        if len(current_window) + len(para) < max_size:\n            current_window += para + \"\\n\\n\"\n        else:\n            if current_window: windows.append(current_window)\n            current_window = para + \"\\n\\n\"\n    if current_window: windows.append(current_window)\n    return windows\n\ndef get_chunking_prompt(book_meta, text_window, prev_context):\n    title = book_meta.get('title', 'Unknown Title')\n    \n    # 1. Location Context\n    loc_str = \"Start of document.\"\n    if prev_context[\"file_id\"] != 1 or prev_context[\"tg_msg_id\"] != 0:\n        loc_str = f\"[File: {prev_context['file_id']} | Book: {prev_context['book_page']} | Ref: {prev_context['tg_msg_id']}]\"\n\n    # 2. FULL TEXT Context (Overlap)\n    last_text_context = \"No previous text.\"\n    if prev_context.get(\"last_chunk_text\"):\n        safe_text = prev_context['last_chunk_text'].replace('\"', \"'\")\n        last_text_context = f\"\\\"{safe_text}\\\"\"\n\n    return f\"\"\"\n    ROLE: Database Engineer.\n    TASK: Split text into SMALL SEMANTIC CHUNKS (Strict limit: {MAX_CHARS_PER_CHUNK} chars).\n    \n    DOCUMENT INFO:\n    Title: \"{title}\"\n    \n    CONTEXT FROM PREVIOUS BATCH:\n    - Last Location: {loc_str}\n    - TEXT OF LAST PROCESSED CHUNK: \n      {last_text_context}\n    \n    INPUT TEXT (contains markers like `> [File: 12 | Book: 45 | Ref: 10050]`):\n    \\\"\\\"\\\"{text_window}\\\"\\\"\\\"\n    \n    INSTRUCTIONS:\n    1. **Identify Entities**:\n       - `content`, `bibliography`, `footnotes_definitions`.\n    \n    2. **STRICT CHUNKING RULES (HARD LIMIT)**:\n       - **ABSOLUTE LIMIT**: No single `chunk_content` can exceed {MAX_CHARS_PER_CHUNK} characters.\n       - **MANDATORY SPLITTING**: If a logical block (e.g. a long article section) is longer than {MAX_CHARS_PER_CHUNK} chars, you **MUST** split it into multiple smaller sequential chunks.\n       - **SPLIT STRATEGY**: Split at paragraph or sentence boundaries. Do not cut in the middle of a sentence.\n    \n    3. **SMART CONTINUITY (CRITICAL)**:\n       - Compare `INPUT TEXT` start with `TEXT OF LAST PROCESSED CHUNK`.\n       - If `INPUT TEXT` continues a broken sentence or thought from the previous chunk:\n         **MERGE** the necessary context from the previous chunk into the start of your first new chunk.\n       \n    4. **Locator Extraction (Memory Injection)**:\n       - Find NEAREST PRECEDING marker `> [File: X | Book: Y | Ref: Z]`.\n       - If NO marker found in window, **USE 'Last Location' from Context**.\n       \n    5. **Relevance & Language**:\n       - Score (0.0-1.0) relevance to \"{title}\".\n       - `semantic_header`/`keywords` in RUSSIAN (or dominant lang).\n    \n    OUTPUT JSON FORMAT (Array):\n    [\n      {{\n        \"chunk_type\": \"content\",\n        \"chunk_content\": \"string (< {MAX_CHARS_PER_CHUNK} chars)\",\n        \"file_id\": int or null,\n        \"book_page\": \"string\" or null,\n        \"tg_msg_id\": int or null, \n        \"relevance_score\": float,\n        \"lang\": \"code\",\n        \"semantic_header\": \"string\",\n        \"keywords\": [\"tag1\"]\n      }}\n    ]\n    \"\"\"\n\nasync def process_rag_pipeline():\n    all_md_files = sorted(glob.glob(os.path.join(INPUT_DIR, \"*.md\")))\n    valid_files = [f for f in all_md_files if \"RAG_DATA\" not in f]\n    \n    if not valid_files:\n        print(\"‚ùå No source MD files found.\")\n        return\n\n    files_to_process = valid_files[:BOOKS_LIMIT]\n    print(f\"üìÇ Found {len(valid_files)}. Processing: {len(files_to_process)}.\")\n    \n    total_in, total_out = 0, 0\n    \n    client = TelegramClient(MemorySession(), API_ID, API_HASH)\n    await client.start(bot_token=BOT_TOKEN)\n    dest_ent = await client.get_entity(DEST_ID)\n    \n    preview_md = f\"# RAG DATA PREVIEW (V59)\\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\"\n    \n    for md_path in files_to_process:\n        filename = os.path.basename(md_path)\n        print(f\"\\nüìò Processing: {filename}\")\n        \n        meta, full_text = parse_processed_md(md_path)\n        if not full_text: continue\n        \n        book_data = {\n            \"metadata\": meta,\n            \"chunks\": [],\n            \"bibliography\": [],\n            \"footnotes\": []\n        }\n            \n        windows = create_smart_windows(full_text, TEXT_WINDOW_SIZE)\n        print(f\"   ‚úÇÔ∏è Windows: {len(windows)}\")\n        \n        context_memory = {\n            \"file_id\": 1,        \n            \"book_page\": \"1\",    \n            \"tg_msg_id\": 0,\n            \"last_chunk_text\": None\n        }\n        \n        for i, window in enumerate(windows):\n            await limiter.wait()\n            \n            model = genai.GenerativeModel(\n                model_name=MODEL_NAME,\n                generation_config={\"response_mime_type\": \"application/json\", \"temperature\": 0.0}\n            )\n            \n            try:\n                print(f\"      üì° Window {i+1}/{len(windows)}...\")\n                prompt = get_chunking_prompt(meta, window, context_memory)\n                \n                response = await asyncio.to_thread(model.generate_content, prompt)\n                \n                if response.usage_metadata:\n                    t_in = response.usage_metadata.prompt_token_count\n                    t_out = response.usage_metadata.candidates_token_count\n                    total_in += t_in; total_out += t_out\n                    print(f\"      ‚úÖ OK (Tok: {t_in}/{t_out})\")\n                \n                new_items = json.loads(response.text)\n                \n                valid_content_chunks = []\n                \n                for item in new_items:\n                    # 1. Update Memory (Location)\n                    if item.get('file_id') is not None:\n                        context_memory['file_id'] = item['file_id']\n                    if item.get('book_page') is not None:\n                        context_memory['book_page'] = item['book_page']\n                    if item.get('tg_msg_id') is not None:\n                        context_memory['tg_msg_id'] = item['tg_msg_id']\n                    \n                    # 2. Patch item if missing\n                    if item.get('file_id') is None:\n                        item['file_id'] = context_memory['file_id']\n                    if item.get('book_page') is None:\n                        item['book_page'] = context_memory['book_page']\n                    if item.get('tg_msg_id') is None:\n                        item['tg_msg_id'] = context_memory['tg_msg_id']\n\n                    # 3. Sort entities\n                    ctype = item.get('chunk_type', 'content')\n                    if ctype == 'bibliography':\n                        book_data[\"bibliography\"].append(item)\n                    elif ctype == 'footnotes_definitions':\n                        book_data[\"footnotes\"].append(item)\n                    else:\n                        book_data[\"chunks\"].append(item)\n                        if item.get('relevance_score', 0) > 0.5:\n                            valid_content_chunks.append(item)\n                \n                # 4. Update Memory (Text Overlap)\n                if valid_content_chunks:\n                    context_memory['last_chunk_text'] = valid_content_chunks[-1]['chunk_content']\n                    snippet = context_memory['last_chunk_text'][:50].replace('\\n', ' ')\n                    print(f\"      üß† Context updated: '{snippet}...'\")\n                \n            except Exception as e:\n                print(f\"      ‚ùå Error: {e}\")\n\n        # --- PREVIEW GENERATION ---\n        preview_md += f\"# BOOK: {filename}\\n\"\n        \n        # 1. Metadata\n        preview_md += \"## 1. Metadata\\n\"\n        for k, v in book_data[\"metadata\"].items():\n             preview_md += f\"- **{k}**: {v}\\n\"\n        \n        # 2. Chunks\n        preview_md += f\"\\n## 2. Content Chunks ({len(book_data['chunks'])})\\n\"\n        for idx, ch in enumerate(book_data[\"chunks\"]):\n            score = ch.get('relevance_score', 0)\n            icon = \"üü¢\" if score > 0.7 else \"üü°\" if score > 0.4 else \"üî¥\"\n            if score < RELEVANCE_THRESHOLD: icon = \"üóëÔ∏è\"\n            \n            flag = get_flag_emoji(ch.get('lang', ''))\n            \n            loc_str = f\"File: {ch.get('file_id')} | Book: {ch.get('book_page')} | Ref: {ch.get('tg_msg_id')}\"\n            \n            size_icon = \"\"\n            if len(ch.get('chunk_content', '')) > 2100: size_icon = \"‚ö†Ô∏èHUGE\"\n            \n            preview_md += f\"### [{idx+1}] {icon} {flag} {loc_str} {size_icon}\\n\"\n            preview_md += f\"- **Topic**: {ch.get('semantic_header')}\\n\"\n            preview_md += f\"- **Tags**: {', '.join(ch.get('keywords', []))}\\n\"\n            preview_md += f\"- **Score**: {score} | **Len**: {len(ch.get('chunk_content', ''))}\\n\"\n            preview_md += f\"```text\\n{ch.get('chunk_content')}\\n```\\n\\n\"\n\n        # 3. Bibliography\n        if book_data[\"bibliography\"]:\n            preview_md += f\"\\n## 3. Bibliography ({len(book_data['bibliography'])})\\n\"\n            for bib in book_data[\"bibliography\"]:\n                loc_str = f\"[Ref: {bib.get('tg_msg_id')} | P: {bib.get('book_page')}]\"\n                preview_md += f\"- {loc_str} {bib.get('chunk_content')[:300]}...\\n\"\n\n        # 4. Footnotes\n        if book_data[\"footnotes\"]:\n            preview_md += f\"\\n## 4. Footnotes ({len(book_data['footnotes'])})\\n\"\n            for fn in book_data[\"footnotes\"]:\n                 loc_str = f\"[Ref: {fn.get('tg_msg_id')} | P: {fn.get('book_page')}]\"\n                 preview_md += f\"- {loc_str} {fn.get('chunk_content')}\\n\"\n\n        preview_md += \"---\\n\\n\"\n\n    with open(OUTPUT_PREVIEW_FILE, \"w\", encoding=\"utf-8\") as f:\n        f.write(preview_md)\n    \n    summary = (\n        f\"üìä **RAG Processing V59**\\n\"\n        f\"Sources: {len(files_to_process)}\\n\"\n        f\"Chunks: {len(book_data['chunks'])}\\n\"\n        f\"Tokens: {total_in} in / {total_out} out\"\n    )\n    print(f\"\\n{summary}\")\n    \n    try:\n        await client.send_message(dest_ent, summary)\n        await client.send_file(dest_ent, OUTPUT_PREVIEW_FILE, caption=\"üß† **RAG Data V59 (Strict Limits)**\")\n    except Exception as e:\n        print(f\"TG Upload Error: {e}\")\n        \n    await client.disconnect()\n\n# === RUN ===\nawait process_rag_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T22:50:38.745394Z","iopub.execute_input":"2025-11-27T22:50:38.745739Z","iopub.status.idle":"2025-11-27T22:52:09.698712Z","shell.execute_reply.started":"2025-11-27T22:50:38.745714Z","shell.execute_reply":"2025-11-27T22:52:09.697854Z"}},"outputs":[{"name":"stdout","text":"üìÇ Found 1. Processing: 1.\n\nüìò Processing: –ú–∞—Ä–∫–∏–Ω_–î_–ù–µ–º—Ü—ã_–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥–∞_2009,_OCR.md\n   ‚úÇÔ∏è Windows: 3\n      üì° Window 1/3...\n      ‚úÖ OK (Tok: 2667/3238)\n      üß† Context updated: '–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –Ω–µ...'\n      üì° Window 2/3...\n      ‚úÖ OK (Tok: 2915/3136)\n      üß† Context updated: '**–£–î–ö 355.2**  D. Markin ## GERMANS IN KALININGRAD...'\n      üì° Window 3/3...\n      ‚úÖ OK (Tok: 1089/608)\n\nüìä **RAG Processing V59**\nSources: 1\nChunks: 12\nTokens: 6671 in / 6982 out\n","output_type":"stream"}],"execution_count":7}]}