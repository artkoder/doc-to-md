{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Ğ¯Ñ‡ĞµĞ¹ĞºĞ° 1: ĞŸĞ Ğ˜ĞĞ£Ğ”Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ• ĞĞ‘ĞĞĞ’Ğ›Ğ•ĞĞ˜Ğ• Ğ‘Ğ˜Ğ‘Ğ›Ğ˜ĞĞ¢Ğ•Ğš\n!apt-get update -qq\n!apt-get install -y -qq djvulibre-bin\n# Ğ¤Ğ»Ğ°Ğ³ -U Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ĞµĞ½, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº gemini-1.5-flash\n!pip install -U -q google-generativeai telethon pymupdf tqdm pillow\n!echo \"âœ… Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ ĞĞ‘ĞĞĞ’Ğ›Ğ•ĞĞ«. Ğ¢ĞµĞ¿ĞµÑ€ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ±Ñ‹Ñ‚ÑŒ Ğ²Ğ¸Ğ´Ğ½Ğ°.\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T13:06:40.192672Z","iopub.execute_input":"2025-11-26T13:06:40.192868Z","iopub.status.idle":"2025-11-26T13:07:14.932413Z","shell.execute_reply.started":"2025-11-26T13:06:40.192848Z","shell.execute_reply":"2025-11-26T13:07:14.931230Z"}},"outputs":[{"name":"stdout","text":"W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nSelecting previously unselected package djvulibre-bin.\n(Reading database ... 128639 files and directories currently installed.)\nPreparing to unpack .../djvulibre-bin_3.5.28-2ubuntu0.22.04.1_amd64.deb ...\nUnpacking djvulibre-bin (3.5.28-2ubuntu0.22.04.1) ...\nSelecting previously unselected package libexiv2-27:amd64.\nPreparing to unpack .../libexiv2-27_0.27.5-3ubuntu1_amd64.deb ...\nUnpacking libexiv2-27:amd64 (0.27.5-3ubuntu1) ...\nSelecting previously unselected package libgraphicsmagick-q16-3.\nPreparing to unpack .../libgraphicsmagick-q16-3_1.4+really1.3.38-1ubuntu0.1_amd64.deb ...\nUnpacking libgraphicsmagick-q16-3 (1.4+really1.3.38-1ubuntu0.1) ...\nSelecting previously unselected package libgraphicsmagick++-q16-12.\nPreparing to unpack .../libgraphicsmagick++-q16-12_1.4+really1.3.38-1ubuntu0.1_amd64.deb ...\nUnpacking libgraphicsmagick++-q16-12 (1.4+really1.3.38-1ubuntu0.1) ...\nSelecting previously unselected package pdf2djvu.\nPreparing to unpack .../pdf2djvu_0.9.18.2-1_amd64.deb ...\nUnpacking pdf2djvu (0.9.18.2-1) ...\nSetting up libexiv2-27:amd64 (0.27.5-3ubuntu1) ...\nSetting up djvulibre-bin (3.5.28-2ubuntu0.22.04.1) ...\nSetting up libgraphicsmagick-q16-3 (1.4+really1.3.38-1ubuntu0.1) ...\nSetting up libgraphicsmagick++-q16-12 (1.4+really1.3.38-1ubuntu0.1) ...\nSetting up pdf2djvu (0.9.18.2-1) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m748.5/748.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pyaes (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngradio 5.38.1 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mâœ… Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ ĞĞ‘ĞĞĞ’Ğ›Ğ•ĞĞ«. Ğ¢ĞµĞ¿ĞµÑ€ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ±Ñ‹Ñ‚ÑŒ Ğ²Ğ¸Ğ´Ğ½Ğ°.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Ğ¯Ñ‡ĞµĞ¹ĞºĞ° Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\n\nprint(\"ğŸ” Ğ¡ĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹...\")\ntry:\n    user_secrets = UserSecretsClient()\n    GENAI_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n    genai.configure(api_key=GENAI_KEY)\n    \n    available_models = []\n    for m in genai.list_models():\n        if 'generateContent' in m.supported_generation_methods:\n            print(f\"   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: {m.name}\")\n            available_models.append(m.name)\n            \n    if not available_models:\n        print(\"âŒ ĞĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹! ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ API Key.\")\n    else:\n        print(f\"\\nĞ’ÑĞµĞ³Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾: {len(available_models)}\")\n\nexcept Exception as e:\n    print(f\"âŒ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}\")\n    print(\"ğŸ’¡ Ğ¡ĞĞ’Ğ•Ğ¢: ĞĞ°Ğ¶Ğ¼Ğ¸Ñ‚Ğµ Ğ² Ğ¼ĞµĞ½Ñ 'Run' -> 'Restart Session', Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T13:19:59.658822Z","iopub.execute_input":"2025-11-26T13:19:59.659258Z","iopub.status.idle":"2025-11-26T13:20:04.263801Z","shell.execute_reply.started":"2025-11-26T13:19:59.659197Z","shell.execute_reply":"2025-11-26T13:20:04.262768Z"}},"outputs":[{"name":"stdout","text":"ğŸ” Ğ¡ĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹...\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-pro-vtea-da-csi\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-pro-preview-03-25\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-flash\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-pro-preview-05-06\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-pro-preview-06-05\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-pro\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.0-flash-exp\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.0-flash\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.0-flash-001\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.0-flash-exp-image-generation\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.0-flash-lite-001\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.0-flash-lite\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.0-flash-lite-preview-02-05\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.0-flash-lite-preview\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.0-pro-exp\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.0-pro-exp-02-05\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-exp-1206\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.0-flash-thinking-exp-01-21\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.0-flash-thinking-exp\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.0-flash-thinking-exp-1219\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-flash-preview-tts\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-pro-preview-tts\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/learnlm-2.0-flash-experimental\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemma-3-1b-it\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemma-3-4b-it\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemma-3-12b-it\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemma-3-27b-it\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemma-3n-e4b-it\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemma-3n-e2b-it\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-flash-latest\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-flash-lite-latest\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-pro-latest\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-flash-lite\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-flash-image-preview\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-flash-image\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-flash-preview-09-2025\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-flash-lite-preview-09-2025\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-3-pro-preview\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-3-pro-image-preview\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/nano-banana-pro-preview\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-robotics-er-1.5-preview\n   âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: models/gemini-2.5-computer-use-preview-10-2025\n\nĞ’ÑĞµĞ³Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾: 42\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# === Ğ¯Ğ§Ğ•Ğ™ĞšĞ 2: V46 (SMART COVER GUARD & STRICT SAFETY) ===\nimport os\nimport time\nimport json\nimport asyncio\nimport shutil\nimport re\nimport fitz  # PyMuPDF\nimport subprocess\nimport google.generativeai as genai\nfrom PIL import Image\nfrom datetime import datetime\nfrom telethon import TelegramClient, events, functions, types\nfrom telethon.sessions import MemorySession\nfrom kaggle_secrets import UserSecretsClient\nfrom tqdm.notebook import tqdm\nfrom google.api_core.exceptions import ResourceExhausted, ServiceUnavailable, InternalServerError\n\n# === SAFETY ===\nImage.MAX_IMAGE_PIXELS = None \n\n# === SETTINGS ===\nSOURCE_USERNAME = \"sendDoc39\"\nBATCH_STRATEGY = [5, 3, 1]  \n\n# GLOBAL HARD LIMIT: Minimum seconds between ANY API calls\nGLOBAL_REQUEST_DELAY = 30.0  \n\nZOOM_FACTOR = 2.0          \nMARKER_EMOJI = \"âœ…\"\nMAX_EMPTY_GAP = 5\nSAFETY_LIMIT = 500\n\n# RENDER SETTINGS\nDJVU_TARGET_SIZE = 2560    \nCROP_PADDING = 15          \nSNAP_THRESHOLD = 30        \n\n# --- PRODUCTION SETTINGS ---\nDEBUG_PAGE_LIMIT = 3     \n# ---------------------------\n\nuser_secrets = UserSecretsClient()\nTEMP_DIR = \"./temp_processing\"\nCROPS_DIR = \"./temp_crops\"\n\nLOG_BUFFER = []\n\ndef log(text):\n    tqdm.write(text)\n    LOG_BUFFER.append(text)\n\nasync def send_report(client, dest_entity):\n    report_text = \"\\n\".join(LOG_BUFFER)\n    if len(report_text) > 4000: report_text = report_text[-3900:]\n    try:\n        if dest_entity: await client.send_message(dest_entity, f\"ğŸ“ **Report:**\\n\\n...{report_text}\")\n    except: pass\n\n# === MODEL MANAGER (HARD THROTTLE) ===\nclass ModelRotator:\n    def __init__(self, api_keys):\n        if isinstance(api_keys, str): self.api_keys = [api_keys]\n        else: self.api_keys = api_keys\n        self.current_key_idx = 0\n        self._configure_current_key()\n        self.last_request_ts = 0\n        \n        self.models_list = [\n            \"models/gemini-2.5-flash\",      # Priority 1\n            \"models/gemini-2.0-flash\",      # Priority 2\n        ]\n        log(f\"ğŸ§  Models initialized. Delay set to {GLOBAL_REQUEST_DELAY}s.\")\n\n    def _configure_current_key(self):\n        genai.configure(api_key=self.api_keys[self.current_key_idx])\n\n    async def _enforce_global_pace(self):\n        now = time.time()\n        elapsed = now - self.last_request_ts\n        if elapsed < GLOBAL_REQUEST_DELAY:\n            wait_time = GLOBAL_REQUEST_DELAY - elapsed + 1\n            await asyncio.sleep(wait_time)\n        self.last_request_ts = time.time()\n\n    async def generate_content(self, content):\n        for i, model_name in enumerate(self.models_list):\n            await self._enforce_global_pace()\n            \n            model = genai.GenerativeModel(\n                model_name=model_name,\n                safety_settings=[{\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"}],\n                generation_config={\"response_mime_type\": \"application/json\", \"temperature\": 0.1}\n            )\n            \n            try:\n                log(f\"      ğŸ“¡ [{model_name}] Sending...\")\n                t_start = time.time()\n                response = await asyncio.to_thread(model.generate_content, content)\n                dur = time.time() - t_start\n                \n                usage = {\"in\": 0, \"out\": 0}\n                if response.usage_metadata:\n                    usage[\"in\"] = response.usage_metadata.prompt_token_count\n                    usage[\"out\"] = response.usage_metadata.candidates_token_count\n                \n                return response.text, usage, model_name, dur\n            \n            except (ResourceExhausted, ServiceUnavailable, InternalServerError) as e:\n                log(f\"      âš ï¸ Limit/Error on {model_name}.\")\n                if i == len(self.models_list) - 1:\n                    log(\"      âŒ ALL MODELS EXHAUSTED. Aborting execution.\")\n                    raise e \n                log(\"      â¡ï¸ Switching to backup model...\")\n                continue\n                \n            except Exception as e:\n                if \"429\" in str(e):\n                    log(f\"      âš ï¸ 429 Too Many Requests on {model_name}.\")\n                    if i == len(self.models_list) - 1:\n                        log(\"      âŒ ALL MODELS EXHAUSTED (429). Aborting.\")\n                        raise e\n                    continue\n                log(f\"      âŒ Unexpected Error ({model_name}): {e}\")\n                raise e\n\n        raise Exception(\"Unexpected logic flow in ModelRotator\")\n\n# === INIT ===\nmodel_rotator = None\ntry:\n    GENAI_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n    model_rotator = ModelRotator(GENAI_KEY)\nexcept Exception as e: log(f\"âŒ Init Error: {e}\")\n\n# === AI LOGIC ===\ndef get_gemini_prompt(is_first_batches, previous_context=None):\n    base_prompt = \"\"\"\n    ROLE: Expert OCR & Document Analyst.\n    TASK: Convert book pages into Structured JSON.\n    \"\"\"\n    \n    if previous_context:\n        base_prompt += f\"\"\"\n    CONTEXT FROM PREVIOUS BATCH:\n    \"...{previous_context}...\"\n    INSTRUCTION: The text above is the end of the previous page. Use it to check for broken sentences.\n    \"\"\"\n\n    base_prompt += \"\"\"\n    OUTPUT FORMAT:\n    {\n      \"pages\": [\n        {\n           \"page_number\": int,\n           \"main_text\": \"string\",       // MARKDOWN.\n           \"marginalia\": \"string\",      // Technical headers/footers\n           \"footnotes\": \"string\",       // Footnotes\n           \"media_objects\": [\n              {\n                 \"id\": \"string\",        // For Cover use \"book_cover\"\n                 \"coordinates\": [ymin, xmin, ymax, xmax], \n                 \"type\": \"string\",      // \"image\", \"table\", \"cover\"\n                 \"caption_ru\": \"string\" \n              }\n           ]\n        }\n      ],\n    \"\"\"\n    \n    if is_first_batches:\n        base_prompt += \"\"\"\n      \"bibliographic_data\": {\n          \"title\": \"string\",\n          \"authors\": [\"string\"],        \n          \"publisher\": \"string\",\n          \"year\": \"string\",\n          \"isbn\": \"string\",\n          \"copyright_holder\": \"string\"\n      }\n    }\n    \"\"\"\n    else:\n        base_prompt += '  \"batch_metadata\": { \"info\": \"continuation\" }\\n}'\n\n    base_prompt += \"\"\"\n    RULES:\n    1. **COVER PAGE (CRITICAL)**: \n       - If Page 1 is the Cover, return EXACTLY ONE media object with type=\"cover\" and id=\"book_cover\".\n       - Set coordinates to [0,0,1000,1000].\n       - DO NOT split the cover into title/logo/author images. Keep it whole.\n\n    2. **SMART HEADERS**: \n       - Mark headers with `##` or `###`.\n       - Look for bold, centered, or separate lines introducing topics.\n\n    3. **TEXT CLEANING**: \n       - **De-hyphenate:** \"con-\\ntinue\" -> \"continue\".\n       - **Reflow:** Remove artificial newlines.\n       - **Columns:** Read logical flow (Col 1 -> Col 2).\n\n    4. **INLINE NOTES**:\n       - Keep notes inside `main_text` as `> Note: ...`. Do NOT move to footnotes.\n\n    5. **COORDINATES**:\n       - Capture FULL image content (0-1000 scale).\n    \"\"\"\n    return base_prompt\n\nasync def process_single_batch(image_paths, page_nums, prev_ctx_text):\n    if not model_rotator: raise Exception(\"No models\")\n    \n    is_start_of_book = (page_nums[0] <= 10)\n    prompt = get_gemini_prompt(is_start_of_book, prev_ctx_text)\n    content = [prompt]\n    loaded_imgs = []\n    \n    for p in image_paths:\n        img = Image.open(p)\n        w, h = img.size\n        if w > 3072 or h > 3072:\n            img.thumbnail((3072, 3072), Image.Resampling.LANCZOS)\n        loaded_imgs.append(img)\n        content.append(img)\n        \n    text_resp, usage, model_name, duration = await model_rotator.generate_content(content)\n    \n    clean_text = text_resp.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n    if clean_text.startswith(\"json\"): clean_text = clean_text[4:].strip()\n    match = re.search(r'\\{.*\\}', clean_text, re.DOTALL)\n    if match: clean_text = match.group(0)\n\n    data = json.loads(clean_text) \n    \n    batch_meta = data.get(\"bibliographic_data\", data.get(\"batch_metadata\", None))\n    return data.get(\"pages\", []), loaded_imgs, usage, model_name, duration, batch_meta\n\ndef crop_images(pil_imgs, pages_data):\n    crops = []\n    if not os.path.exists(CROPS_DIR): os.makedirs(CROPS_DIR, exist_ok=True)\n    \n    for i, page_data in enumerate(pages_data):\n        if i >= len(pil_imgs): break\n        media_list = page_data.get(\"media_objects\", [])\n        src = pil_imgs[i]\n        w, h = src.size\n        \n        # === COVER GUARD LOGIC ===\n        # Check if ANY object on this page is marked as cover\n        is_cover_page = False\n        cover_object = None\n        \n        for m in media_list:\n            if m.get(\"type\") == \"cover\" or m.get(\"id\") == \"book_cover\":\n                is_cover_page = True\n                cover_object = m\n                break\n        \n        if is_cover_page and cover_object:\n            # If cover detected, IGNORE all other objects. \n            # Force Full Page Crop.\n            cover_object[\"coordinates\"] = [0, 0, 1000, 1000]\n            cover_object[\"id\"] = \"book_cover\"\n            media_list = [cover_object]\n        # =========================\n        \n        for meta in media_list:\n            try:\n                ymin, xmin, ymax, xmax = meta[\"coordinates\"]\n                \n                left = (xmin/1000)*w - CROP_PADDING\n                top = (ymin/1000)*h - CROP_PADDING\n                right = (xmax/1000)*w + CROP_PADDING\n                bottom = (ymax/1000)*h + CROP_PADDING\n                \n                if left < SNAP_THRESHOLD: left = 0\n                if top < SNAP_THRESHOLD: top = 0\n                if right > (w - SNAP_THRESHOLD): right = w\n                if bottom > (h - SNAP_THRESHOLD): bottom = h\n                \n                left = max(0, left); top = max(0, top)\n                right = min(w, right); bottom = min(h, bottom)\n                \n                if (right-left < 30) or (bottom-top < 30): continue\n                \n                crop = src.crop((left, top, right, bottom))\n                \n                raw_id = meta.get(\"id\", f\"media_{int(time.time())}_{i}\")\n                # Sanitize ID\n                clean_id = re.sub(r'[^a-zA-Z0-9_]', '', raw_id)\n                \n                c_name = f\"{clean_id}.png\"\n                c_path = os.path.join(CROPS_DIR, c_name)\n                crop.save(c_path)\n                \n                caption = f\"{meta.get('type', 'img').upper()}: {meta.get('caption_ru', '')}\"\n                crops.append((c_path, caption, clean_id))\n            except: pass\n    return crops\n\n# === TELEGRAM & UTILS ===\nasync def get_last_id(client, entity):\n    try:\n        full = await client(functions.channels.GetFullChannelRequest(entity))\n        match = re.search(r\"\\[LastID:\\s*(\\d+)\\]\", full.full_chat.about or \"\")\n        if match: return int(match.group(1))\n    except: pass\n    return None\n\nasync def update_last_id(client, entity, new_id):\n    try:\n        full = await client(functions.channels.GetFullChannelRequest(entity))\n        about = full.full_chat.about or \"\"\n        tag = f\"[LastID: {new_id}]\"\n        if \"[LastID:\" in about: new_about = re.sub(r\"\\[LastID:\\s*\\d+\\]\", tag, about)\n        else: new_about = f\"{about}\\n\\n{tag}\".strip()\n        if new_about != about: await client(functions.messages.EditChatAboutRequest(peer=entity, about=new_about))\n    except: pass\n\nasync def is_already_processed(message):\n    try:\n        if not message.reactions: return False\n        for r in message.reactions.results:\n            emoji = r.reaction.emoticon if hasattr(r.reaction, 'emoticon') else str(r.reaction)\n            if emoji == MARKER_EMOJI and r.chosen: return True\n    except: pass\n    return False\n\nasync def mark_processed(client, msg):\n    try:\n        await client(functions.messages.SendReactionRequest(peer=msg.peer_id, msg_id=msg.id, reaction=[types.ReactionEmoji(emoticon=MARKER_EMOJI)]))\n    except: pass\n\ndef render_pdf(path):\n    doc = fitz.open(path)\n    mat = fitz.Matrix(ZOOM_FACTOR, ZOOM_FACTOR)\n    limit = len(doc)\n    if DEBUG_PAGE_LIMIT:\n        limit = min(len(doc), DEBUG_PAGE_LIMIT)\n        log(f\"   ğŸ›  DEBUG: Processing first {limit} pages only.\")\n    for i in range(limit):\n        page = doc.load_page(i)\n        pix = page.get_pixmap(matrix=mat, alpha=False)\n        out = os.path.join(TEMP_DIR, f\"page_{i+1:04d}.png\")\n        pix.save(out)\n        yield out, i+1, len(doc)\n    doc.close()\n\ndef render_djvu(path):\n    safe = os.path.join(TEMP_DIR, \"temp.djvu\")\n    shutil.move(path, safe)\n    \n    log(\"   â³ Rasterizing DjVu (subprocess)...\")\n    try:\n        subprocess.run([\n            'ddjvu', '-format=tiff', f'-size={DJVU_TARGET_SIZE}x{DJVU_TARGET_SIZE}', \n            '-eachpage', safe, os.path.join(TEMP_DIR, \"p_%04d.tif\")\n        ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n    except Exception as e:\n        log(f\"âŒ DjVu Render Error: {e}\")\n        return\n\n    files = sorted([f for f in os.listdir(TEMP_DIR) if f.endswith(\".tif\")])\n    if DEBUG_PAGE_LIMIT: files = files[:DEBUG_PAGE_LIMIT]\n    \n    log(f\"   ğŸ–¼ Converting {len(files)} pages to PNG...\")\n    for i, f in enumerate(tqdm(files, desc=\"Conv\", unit=\"pg\", leave=False)):\n        full = os.path.join(TEMP_DIR, f)\n        try:\n            with Image.open(full) as img:\n                png = full.replace(\".tif\", \".png\")\n                img.save(png, \"PNG\")\n                yield png, i+1, len(files)\n            os.remove(full)\n        except: pass\n    if os.path.exists(safe): os.remove(safe)\n\n# === MAIN ===\nasync def main_logic():\n    log(f\"ğŸš€ **Launch V46 (Smart Cover Guard):** {datetime.now().strftime('%H:%M:%S')}\")\n    \n    try:\n        API_ID = int(user_secrets.get_secret(\"TELEGRAM_API_ID\"))\n        API_HASH = user_secrets.get_secret(\"TELEGRAM_API_HASH\")\n        BOT_TOKEN = user_secrets.get_secret(\"TELEGRAM_BOT_TOKEN\")\n        DEST_ID = int(user_secrets.get_secret(\"DEST_CHANNEL_ID\"))\n    except: return log(\"âŒ Secrets missing.\")\n\n    for d in [TEMP_DIR, CROPS_DIR]:\n        if os.path.exists(d): shutil.rmtree(d); os.makedirs(d, exist_ok=True)\n\n    client = TelegramClient(MemorySession(), API_ID, API_HASH)\n    await client.start(bot_token=BOT_TOKEN)\n    log(\"âœ… Telegram OK.\")\n    if not model_rotator: return await client.disconnect()\n\n    try:\n        src_ent = await client.get_entity(SOURCE_USERNAME)\n        dst_ent = await client.get_entity(DEST_ID)\n    except: return await client.disconnect()\n\n    start_id = await get_last_id(client, src_ent) or 1\n    curr_id = start_id + 1\n    processed = 0\n    empty_streak = 0\n    t_in, t_out = 0, 0\n    \n    pbar = tqdm(total=SAFETY_LIMIT, desc=\"Scan\", unit=\"msg\")\n\n    while processed < SAFETY_LIMIT:\n        if empty_streak >= MAX_EMPTY_GAP: log(\"ğŸ End.\"); break\n        try:\n            msgs = await client.get_messages(src_ent, ids=[curr_id])\n            msg = msgs[0] if msgs else None\n            \n            if not msg or isinstance(msg, types.MessageEmpty):\n                empty_streak += 1; curr_id += 1; pbar.update(1); continue\n            \n            if await is_already_processed(msg):\n                await update_last_id(client, src_ent, msg.id)\n                curr_id += 1; pbar.update(1); continue\n\n            fname = getattr(msg.file, 'name', None)\n            if not fname: curr_id += 1; pbar.update(1); continue\n            ext = fname.split('.')[-1].lower()\n            if ext not in ['pdf', 'djvu']: curr_id += 1; pbar.update(1); continue\n\n            empty_streak = 0\n            log(f\"\\nğŸ“˜ **{fname}** (ID: {msg.id})\")\n            fpath = os.path.join(TEMP_DIR, fname)\n            await msg.download_media(file=fpath)\n            \n            iter_pages = render_pdf(fpath) if ext == 'pdf' else render_djvu(fpath)\n            \n            # --- GLOBAL ACCUMULATORS ---\n            global_text_blocks = []     \n            global_resources = []       \n            global_footnotes = []\n            global_marginalia = []\n            full_bibliographic_data = {}\n            \n            last_batch_tail_text = None \n            \n            # --- ADAPTIVE LOOP ---\n            all_page_paths = []\n            all_page_idxs = []\n            \n            current_batch_size = BATCH_STRATEGY[0]\n            \n            generated_pages_buffer = []\n            for p in iter_pages: generated_pages_buffer.append(p)\n            \n            cursor = 0\n            total_pages = len(generated_pages_buffer)\n            \n            book_failed = False\n\n            while cursor < total_pages:\n                end = min(cursor + current_batch_size, total_pages)\n                current_slice = generated_pages_buffer[cursor : end]\n                slice_paths = [x[0] for x in current_slice]\n                slice_idxs = [x[1] for x in current_slice]\n                \n                log(f\"   ğŸ”„ Batch {slice_idxs[0]}-{slice_idxs[-1]} (Size: {len(slice_paths)})...\")\n                \n                try:\n                    pages_list, imgs, usg, m_name, duration, meta = await process_single_batch(\n                        slice_paths, slice_idxs, last_batch_tail_text\n                    )\n                    \n                    log(f\"      âœ… [{m_name}] OK in {duration:.1f}s\")\n                    t_in += usg.get(\"in\",0); t_out += usg.get(\"out\",0)\n                    \n                    try: await client.send_file(dst_ent, slice_paths, force_document=True)\n                    except: pass\n\n                    if meta and isinstance(meta, dict):\n                        for k, v in meta.items():\n                            if v and not full_bibliographic_data.get(k):\n                                full_bibliographic_data[k] = v\n                        log(f\"      â„¹ï¸ Meta updated. ISBN: {full_bibliographic_data.get('isbn', 'No')}\")\n\n                    crops = crop_images(imgs, pages_list)\n                    for cp_path, cp_cap, cp_id in crops:\n                        msg_link = \"local_error\"\n                        if os.path.exists(cp_path):\n                            try:\n                                c_msg = await client.send_file(dst_ent, cp_path, caption=f\"ğŸ–¼ {cp_cap}\\nğŸ†” {cp_id}\", force_document=False)\n                                msg_link = f\"tg://msg_id?id={c_msg.id}\"\n                            except: pass\n                            finally:\n                                if os.path.exists(cp_path): os.remove(cp_path)\n                                await asyncio.sleep(2)\n                        global_resources.append({\"id\": cp_id, \"caption\": cp_cap, \"link\": msg_link})\n\n                    for i, page_data in enumerate(pages_list):\n                        raw_text = page_data.get(\"main_text\", \"\")\n                        p_num = page_data.get(\"page_number\", slice_idxs[i] if i < len(slice_idxs) else \"?\")\n                        \n                        if page_data.get(\"footnotes\"): global_footnotes.append(page_data[\"footnotes\"])\n                        if page_data.get(\"marginalia\"): global_marginalia.append(page_data[\"marginalia\"])\n                        \n                        global_text_blocks.append({\"p\": p_num, \"text\": raw_text.strip()})\n                    \n                    if global_text_blocks:\n                        last_text_content = global_text_blocks[-1][\"text\"]\n                        last_batch_tail_text = last_text_content[-1000:]\n                    \n                    for p in slice_paths:\n                        if os.path.exists(p): os.remove(p)\n                        \n                    cursor += len(slice_paths)\n                    \n                except Exception as e:\n                    log(f\"      âš ï¸ Failed: {str(e)[:100]}\")\n                    \n                    if \"ResourceExhausted\" in str(e) or \"429\" in str(e):\n                        log(\"      âŒ CRITICAL API LIMIT. Stopping book processing.\")\n                        book_failed = True\n                        break\n\n                    new_size = current_batch_size\n                    for s in BATCH_STRATEGY:\n                        if s < current_batch_size:\n                            new_size = s\n                            break\n                    \n                    if new_size == current_batch_size:\n                        log(f\"      âŒ PARSE ERROR: Page {slice_idxs[0]} failed at size 1. Skipping.\")\n                        cursor += 1 \n                        current_batch_size = 1\n                    else:\n                        log(f\"      ğŸ“‰ Downgrading batch size: {current_batch_size} -> {new_size}\")\n                        current_batch_size = new_size\n\n            if book_failed:\n                await client.send_message(dst_ent, f\"âŒ **Aborted:** {fname} (API Limits)\")\n                await update_last_id(client, src_ent, msg.id)\n                continue\n\n            # === GLOBAL ASSEMBLY ===\n            final_md = f\"# {fname}\\n\\n\"\n            \n            if full_bibliographic_data:\n                final_md += \"## Metadata\\n\"\n                for k, v in full_bibliographic_data.items():\n                    if v: \n                        val_str = \", \".join(v) if isinstance(v, list) else str(v)\n                        final_md += f\"- **{k}**: {val_str}\\n\"\n                final_md += \"\\n\"\n            \n            final_md += \"## Content\\n\\n\"\n            \n            joined_content = \"\"\n            for i, block_data in enumerate(global_text_blocks):\n                p_num = block_data[\"p\"]\n                text = block_data[\"text\"]\n                page_marker = f\"\\n\\n> `[Page {p_num}]`\\n\"\n                \n                if i > 0:\n                    prev_text = global_text_blocks[i-1][\"text\"]\n                    if prev_text and prev_text[-1] not in \".!?\\\"'â€\":\n                        joined_content += page_marker + text\n                    else:\n                        joined_content += page_marker + text\n                else:\n                    joined_content += f\"> `[Page {p_num}]`\\n\" + text\n            \n            final_md += joined_content.strip()\n\n            if global_resources:\n                final_md += \"\\n\\n## APPX: Resources\\n\"\n                for res in global_resources:\n                    final_md += f\"- **ID**: `{res['id']}`\\n\"\n                    final_md += f\"  - **Caption**: {res['caption']}\\n\"\n                    final_md += f\"  - **Link**: {res['link']}\\n\"\n            \n            if global_footnotes:\n                final_md += \"\\n\\n## APPX: Footnotes\\n\" + \"\\n\\n\".join(global_footnotes)\n            if global_marginalia:\n                final_md += \"\\n\\n## APPX: Marginalia\\n\" + \"\\n\".join(global_marginalia)\n\n            md_out = os.path.join(TEMP_DIR, f\"{os.path.splitext(fname)[0]}.md\")\n            with open(md_out, \"w\", encoding=\"utf-8\") as f: f.write(final_md)\n            await client.send_file(dst_ent, md_out, caption=f\"ğŸ“š **MD FULL** (Tok: {t_in+t_out})\", force_document=True)\n            \n            await mark_processed(client, msg)\n            await update_last_id(client, src_ent, msg.id)\n            log(f\"   ğŸ‰ Done.\")\n\n        except Exception as e:\n            log(f\"âŒ Error ID {curr_id}: {e}\")\n            import traceback; traceback.print_exc()\n        \n        finally:\n            for d in [TEMP_DIR, CROPS_DIR]:\n                if os.path.exists(d): shutil.rmtree(d); os.makedirs(d, exist_ok=True)\n        curr_id += 1; processed += 1; pbar.update(1)\n\n    log(f\"\\nğŸ End. Total Tokens: {t_in}/{t_out}\")\n    await send_report(client, dst_ent)\n    await client.disconnect()\n\n# === RUN ===\nprint(\"ğŸŸ¡ Pre-flight check...\")\ntry:\n    _ = UserSecretsClient().get_secret(\"TELEGRAM_API_ID\")\n    print(\"ğŸŸ¢ Starting main_logic...\")\n    await main_logic()\nexcept Exception as e: print(f\"ğŸ”´ FAIL: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T13:53:08.221063Z","iopub.execute_input":"2025-11-26T13:53:08.221682Z","iopub.status.idle":"2025-11-26T13:57:57.178290Z","shell.execute_reply.started":"2025-11-26T13:53:08.221648Z","shell.execute_reply":"2025-11-26T13:57:57.177268Z"}},"outputs":[{"name":"stdout","text":"ğŸ§  Models initialized. Delay set to 30.0s.\nğŸŸ¡ Pre-flight check...\nğŸŸ¢ Starting main_logic...\nğŸš€ **Launch V46 (Smart Cover Guard):** 13:53:08\nâœ… Telegram OK.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Scan:   0%|          | 0/500 [00:00<?, ?msg/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcbeacee3b344ce28a0f17d8af7e964c"}},"metadata":{}},{"name":"stdout","text":"\nğŸ“˜ **Ğ“Ñ€Ğ°Ñ†Ğ¸Ğ°Ğ½ÑĞºĞ¸Ğ¹ Ğ. ĞšÑ‘Ğ½Ğ¸Ğ³ÑĞ±ĞµÑ€Ğ³ (1947), OCR.pdf** (ID: 39)\n   ğŸ›  DEBUG: Processing first 3 pages only.\n   ğŸ”„ Batch 1-3 (Size: 3)...\n      ğŸ“¡ [models/gemini-2.5-flash] Sending...\n      âœ… [models/gemini-2.5-flash] OK in 200.9s\n      â„¹ï¸ Meta updated. ISBN: No\n   ğŸ‰ Done.\n\nğŸ“˜ **ĞœĞ°Ñ€ĞºĞ¸Ğ½_Ğ”_ĞĞµĞ¼Ñ†Ñ‹_ĞšĞ°Ğ»Ğ¸Ğ½Ğ¸Ğ½Ğ³Ñ€Ğ°Ğ´Ğ°_2009,_OCR.pdf** (ID: 40)\n   ğŸ›  DEBUG: Processing first 3 pages only.\n   ğŸ”„ Batch 1-3 (Size: 3)...\n      ğŸ“¡ [models/gemini-2.5-flash] Sending...\n      âœ… [models/gemini-2.5-flash] OK in 64.1s\n      â„¹ï¸ Meta updated. ISBN: No\n   ğŸ‰ Done.\nğŸ End.\n\nğŸ End. Total Tokens: 2546/4728\n","output_type":"stream"}],"execution_count":4}]}